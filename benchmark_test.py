# -*- coding: utf-8 -*-
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F

# ä½ çš„é¡¹ç›®é‡Œåº”è¯¥æ˜¯ç±»ä¼¼è¿™æ ·å¯¼å…¥ï¼ˆæŒ‰ä½ ä»“åº“å®é™…è·¯å¾„è°ƒæ•´ï¼‰
# from model.network import DSTCarbonFormer
# from model.losses import HybridLoss
from models.network import DSTCarbonFormer
from models.losses import HybridLoss


torch.backends.cudnn.benchmark = True  # ROCm ä¸‹ä¸ä¸€å®šç­‰ä»·ï¼Œä½†ä¿ç•™æ— å¦¨


def _sync():
    torch.cuda.synchronize()


def _timeit(fn):
    _sync()
    t0 = time.time()
    fn()
    _sync()
    return (time.time() - t0) * 1000.0


@torch.no_grad()
def benchmark_forward_only(module: nn.Module, x, name: str, iters=10, warmup=3):
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å— (forward-only): {name}")
    print("   ğŸ”¥ é¢„çƒ­ä¸­...")
    for _ in range(warmup):
        _ = module(x)
    _sync()

    total = 0.0
    for _ in range(iters):
        total += _timeit(lambda: module(x))
    avg = total / iters
    print(f"   â±ï¸ å¹³å‡è€—æ—¶: {avg:.2f} ms / batch")
    print("--------------------------------------------------")
    return avg


def benchmark_trainstep(module: nn.Module, x, target, name: str, iters=10, warmup=3, amp=True):
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å— (trainstep): {name}")
    module.train()
    opt = torch.optim.Adam(module.parameters(), lr=1e-3)
    scaler = torch.cuda.amp.GradScaler(enabled=amp)

    # warmup
    for _ in range(warmup):
        opt.zero_grad(set_to_none=True)
        with torch.amp.autocast("cuda", enabled=amp):
            y = module(x)
            loss = F.mse_loss(y, target)
        scaler.scale(loss).backward()
        scaler.step(opt)
        scaler.update()
    _sync()

    fwd_ms = loss_ms = bwd_ms = step_ms = 0.0
    for _ in range(iters):
        opt.zero_grad(set_to_none=True)

        # fwd
        def _fwd():
            nonlocal y
            with torch.amp.autocast("cuda", enabled=amp):
                y = module(x)

        y = None
        fwd_ms += _timeit(_fwd)

        # loss
        def _loss():
            nonlocal loss
            loss = F.mse_loss(y, target)

        loss = None
        loss_ms += _timeit(_loss)

        # bwd
        def _bwd():
            scaler.scale(loss).backward()

        bwd_ms += _timeit(_bwd)

        # step
        def _step():
            scaler.step(opt)
            scaler.update()

        step_ms += _timeit(_step)

    fwd_ms /= iters
    loss_ms /= iters
    bwd_ms /= iters
    step_ms /= iters
    total_ms = fwd_ms + loss_ms + bwd_ms + step_ms

    print(f"   â±ï¸ fwd : {fwd_ms:.2f} ms")
    print(f"   â±ï¸ loss: {loss_ms:.2f} ms")
    print(f"   â±ï¸ bwd : {bwd_ms:.2f} ms")
    print(f"   â±ï¸ step: {step_ms:.2f} ms")
    print(f"   âœ… total: {total_ms:.2f} ms / iter")
    print("--------------------------------------------------")
    return total_ms


def benchmark_full_model_trainstep(
    model: nn.Module,
    criterion: nn.Module,
    aux, main, target,
    name: str,
    constraint_scale,
    iters=5,
    warmup=1,
    amp=True,
):
    print(f"ğŸ§ª å…¨æµç¨‹æµ‹è¯• (Full Model + Loss): {name}")
    print(f"   âš™ï¸ constraint_scale={constraint_scale} | amp={amp}")
    model.train()
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)
    scaler = torch.amp.GradScaler("cuda", enabled=amp)

    print("   ğŸ”¥ é¢„çƒ­ä¸­ (å«åä¼ )...")
    for _ in range(warmup):
        opt.zero_grad(set_to_none=True)
        with torch.amp.autocast("cuda", enabled=amp):
            pred, pred_raw = model(aux, main, constraint_scale=constraint_scale)
            loss = criterion(pred, target, aux, pred_raw=pred_raw)
        scaler.scale(loss).backward()
        scaler.step(opt)
        scaler.update()
    _sync()

    total_ms = 0.0
    for _ in range(iters):
        opt.zero_grad(set_to_none=True)

        # model fwd
        def _mfwd():
            nonlocal pred, pred_raw
            with torch.amp.autocast("cuda", enabled=amp):
                pred, pred_raw = model(aux, main, constraint_scale=constraint_scale)

        pred = pred_raw = None
        t_fwd = _timeit(_mfwd)

        # loss
        def _l():
            nonlocal loss
            loss = criterion(pred, target, aux, pred_raw=pred_raw)

        loss = None
        t_loss = _timeit(_l)

        # backward
        t_bwd = _timeit(lambda: scaler.scale(loss).backward())

        # step
        def _st():
            scaler.step(opt)
            scaler.update()

        t_step = _timeit(_st)

        t_total = t_fwd + t_loss + t_bwd + t_step
        total_ms += t_total

        print(f"   â±ï¸ Model Fwd : {t_fwd:.2f} ms")
        print(f"   â±ï¸ Loss Calc : {t_loss:.2f} ms")
        print(f"   â±ï¸ Backward  : {t_bwd:.2f} ms")
        print(f"   â±ï¸ Opt Step  : {t_step:.2f} ms")
        print(f"   âœ… Total Time: {t_total:.2f} ms / iter")

        if hasattr(criterion, "log_vars"):
            w = torch.exp(-criterion.log_vars.detach()).cpu().numpy()
            print(f"   âš–ï¸ weights(exp(-log_vars)) = {w}")

    avg = total_ms / iters
    return avg


def main():
    device = "cuda"
    gpu_name = torch.cuda.get_device_name(0)
    print(f"ğŸ”¥ ç¡¬ä»¶: {gpu_name}")

    # å‚æ•°ï¼ˆä¸ä½ è¾“å‡ºä¸€è‡´ï¼‰
    B = int(os.environ.get("BATCH", "4"))
    C_AUX = 9
    C_MAIN = 1
    DIM = int(os.environ.get("DIM", "64"))
    H = W = int(os.environ.get("SIZE", "120"))
    T = int(os.environ.get("T", "3"))

    TRAIN_SCALE = int(os.environ.get("CONSTRAINT_SCALE", "120"))
    AMP = os.environ.get("AMP", "1") == "1"

    # âœ… benchmark æ—¶å¯é€‰è·³è¿‡ constraintï¼ˆé»˜è®¤ä¸è·³è¿‡ï¼‰
    # è®¾ä¸º 1 æ—¶ï¼Œä¼šæŠŠ constraint_scale ä¼  Noneï¼Œä»è€Œ network.py é‡Œè·³è¿‡ constraint è®¡ç®—
    SKIP_CONSTRAINT = os.environ.get("BENCH_SKIP_CONSTRAINT", "0") == "1"
    constraint_scale = None if SKIP_CONSTRAINT else TRAIN_SCALE

    print(f"âš™ï¸ æµ‹è¯•å‚æ•°: Batch={B}, Dim={DIM}, Size={H}x{W}, T={T}")
    print(f"âš™ï¸ å…¨æ¨¡å‹ constraint_scale={constraint_scale}")

    # dummy inputs
    aux = torch.randn(B, C_AUX, T, H, W, device=device)
    main_in = torch.rand(B, C_MAIN, T, H, W, device=device)  # log_norm åŸŸï¼Œ>=0
    target = torch.rand(B, C_MAIN, T, H, W, device=device)

    # 3Då·ç§¯æ¨¡å—ï¼ˆç¤ºä¾‹ï¼‰
    conv3d = nn.Conv3d(DIM, DIM, 3, padding=1).to(device)

    # MoEæ¨¡å— / Mambaæ¨¡å— / Fusionæ¨¡å—ï¼šä½ è¿™é‡ŒæŒ‰é¡¹ç›®å®é™…æ„å»º
    # è¿™é‡Œæ²¿ç”¨ä½ ç›®å‰è„šæœ¬å·²æœ‰çš„æ„é€ æ–¹å¼ï¼ˆç•¥ï¼‰â€”â€”é‡ç‚¹æ˜¯ full-model éƒ¨åˆ†
    # å¦‚æœä½ åŸè„šæœ¬é‡Œæœ‰æ›´å®Œæ•´çš„ module æ„é€ ï¼Œè¯·ä¿ç•™å¹¶ä»…æ›¿æ¢ full-model benchmark è°ƒç”¨ä¸ constraint ä¼ å‚é€»è¾‘ã€‚

    # å…¨æ¨¡å‹
    model = DSTCarbonFormer(aux_c=C_AUX, main_c=C_MAIN, dim=DIM).to(device)
    criterion = HybridLoss(consistency_scale=10, norm_factor=11.0).to(device)

    print("=========================")
    print("ğŸš€ å‡†å¤‡è¿›è¡Œ DSTCarbonFormer å…¨æ¨¡å‹æµ‹è¯•...")
    print("--------------------------------------------------")
    avg_ms = benchmark_full_model_trainstep(
        model=model,
        criterion=criterion,
        aux=aux,
        main=main_in,
        target=target,
        name="DSTCarbonFormer + HybridLoss",
        constraint_scale=constraint_scale,
        iters=20,
        warmup=3,
        amp=AMP,
    )
    _ = avg_ms


if __name__ == "__main__":
    main()
