# -*- coding: utf-8 -*-
import os
import gc
import time
import warnings
from typing import Union, Tuple, List, Dict

# ==========================================
# ğŸ”‡ [æ—¥å¿—é™éŸ³]
# ==========================================
warnings.filterwarnings("ignore", message=".*Dynamo does not know how to trace the builtin.*")
warnings.filterwarnings("ignore", message=".*Unable to hit fast path of CUDAGraphs.*")
warnings.filterwarnings("ignore", message=".*TensorFloat32 tensor cores.*")

# ==========================================
# ğŸš€ [ç¯å¢ƒè¡¥ä¸]
# ==========================================
cache_dir = os.path.expanduser("~/.cache/miopen")
os.makedirs(cache_dir, exist_ok=True)
os.environ["MIOPEN_USER_DB_PATH"] = cache_dir
os.environ["MIOPEN_CUSTOM_CACHE_DIR"] = cache_dir
os.environ["MIOPEN_FORCE_USE_WORKSPACE"] = "1"
os.environ["MIOPEN_LOG_LEVEL"] = "4"
os.environ["MIOPEN_DEBUG_CONV_GEMM"] = "0"
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["MKL_SERVICE_FORCE_INTEL"] = "1"

import torch
torch.set_float32_matmul_precision('high')
import torch.nn as nn

torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# ==========================================
# å¯¼å…¥é¡¹ç›®æ¨¡å—
# ==========================================
try:
    from models.blocks import MultiScaleBlock3D, SFTLayer3D, MoEBlock
    from models.network import DSTCarbonFormer
    from mamba_ssm import Mamba
    # ğŸ”¥ æ–°å¢å¯¼å…¥ Loss
    from models.losses import HybridLoss 
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿ losses.py å·²ä¿å­˜åˆ° models/losses.pyï¼Œä¸” blocks.py/network.py å‡å­˜åœ¨ã€‚")
    raise SystemExit(1)

# ... (MambaAdapter ä¿æŒä¸å˜) ...
class MambaAdapter(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.mamba = Mamba(d_model=dim, d_state=16, d_conv=4, expand=2)

    @torch.compiler.disable
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, T, H, W = x.shape
        x_flat = x.flatten(2).transpose(1, 2)
        out = self.mamba(x_flat)
        return out.transpose(1, 2).view(B, C, T, H, W)

# ... (_to_device, _clean ä¿æŒä¸å˜) ...
def _to_device(inputs, device):
    if isinstance(inputs, (tuple, list)):
        return [x.to(device, non_blocking=True) for x in inputs]
    return [inputs.to(device, non_blocking=True)]

def _clean(*objs):
    for o in objs:
        try:
            del o
        except Exception:
            pass
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ... (benchmark_forward ä¿æŒä¸å˜) ...
@torch.no_grad()
def benchmark_forward(name, module, inputs, iters=50, warmup=5):
    print("--------------------------------------------------")
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å— (forward-only): {name}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        module = module.to(device)
        module.eval()
        inps = _to_device(inputs, device)
        
        # é¢„çƒ­
        print("   ğŸ”¥ é¢„çƒ­ä¸­...")
        for _ in range(warmup):
            torch.compiler.cudagraph_mark_step_begin()
            _ = module(*inps)
        if torch.cuda.is_available(): torch.cuda.synchronize()

        # æµ‹è¯•
        start = time.time()
        for _ in range(iters):
            torch.compiler.cudagraph_mark_step_begin()
            _ = module(*inps)
        if torch.cuda.is_available(): torch.cuda.synchronize()

        avg_ms = (time.time() - start) / iters * 1000.0
        print(f"   â±ï¸ å¹³å‡è€—æ—¶: {avg_ms:.2f} ms / batch")
        return avg_ms
    except Exception as e:
        print(f"   âŒ forward-only æµ‹è¯•å¤±è´¥: {e}")
        return float("inf")
    finally:
        _clean(module, inputs)

# ... (benchmark_trainstep ä¿æŒä¸å˜ï¼Œç”¨äºå•æ¨¡å—æµ‹è¯•) ...
def benchmark_trainstep(name, module, inputs, iters=20, warmup=3, lr=1e-4, use_amp=False):
    # (æ­¤å¤„ä»£ç ä¸ä¹‹å‰ä¸€è‡´ï¼Œçœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œé‡ç‚¹æ˜¯ä¸‹é¢çš„ full_model ç‰ˆ)
    print("--------------------------------------------------")
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å— (trainstep): {name}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        module = module.to(device)
        module.train()
        inps = _to_device(inputs, device)
        opt = torch.optim.AdamW(module.parameters(), lr=lr)
        scaler = torch.amp.GradScaler('cuda', enabled=use_amp)

        for _ in range(warmup):
            torch.compiler.cudagraph_mark_step_begin()
            opt.zero_grad(set_to_none=True)
            with torch.amp.autocast('cuda', enabled=use_amp):
                out = module(*inps)
                loss = out.float().mean()
            scaler.scale(loss).backward()
            scaler.step(opt)
            scaler.update()
        if torch.cuda.is_available(): torch.cuda.synchronize()

        fwd_t = loss_t = bwd_t = step_t = 0.0
        for _ in range(iters):
            torch.compiler.cudagraph_mark_step_begin()
            opt.zero_grad(set_to_none=True)
            
            t0 = time.time()
            with torch.amp.autocast('cuda', enabled=use_amp):
                out = module(*inps)
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t1 = time.time()
            
            loss = out.float().mean()
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t2 = time.time()
            
            scaler.scale(loss).backward()
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t3 = time.time()
            
            scaler.step(opt)
            scaler.update()
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t4 = time.time()
            
            fwd_t += (t1 - t0); loss_t += (t2 - t1); bwd_t += (t3 - t2); step_t += (t4 - t3)

        n = iters
        fwd_ms, loss_ms = fwd_t/n*1000, loss_t/n*1000
        bwd_ms, step_ms = bwd_t/n*1000, step_t/n*1000
        total_ms = fwd_ms + loss_ms + bwd_ms + step_ms
        print(f"   â±ï¸ fwd : {fwd_ms:.2f} ms")
        print(f"   â±ï¸ loss: {loss_ms:.2f} ms")
        print(f"   â±ï¸ bwd : {bwd_ms:.2f} ms")
        print(f"   â±ï¸ step: {step_ms:.2f} ms")
        print(f"   âœ… total: {total_ms:.2f} ms / iter")
        return {"total_ms": total_ms, "fwd_ms": fwd_ms, "bwd_ms": bwd_ms}
    except Exception as e:
        print(f"   âŒ trainstep æµ‹è¯•å¤±è´¥: {e}")
        return {"total_ms": float("inf")}
    finally:
        _clean(module, inputs)

# ==========================================
# ğŸ”¥ [æ–°å¢] å…¨æ¨¡å‹ + Loss ä¸“ç”¨æµ‹è¯•å‡½æ•°
# ==========================================
def benchmark_full_model_trainstep(
    name: str,
    model: nn.Module,
    criterion: nn.Module,
    aux_input: torch.Tensor,
    main_input: torch.Tensor,
    target: torch.Tensor,
    iters: int = 20,
    warmup: int = 3,
    lr: float = 1e-4,
    use_amp: bool = True # é»˜è®¤å¼€å¯æ··åˆç²¾åº¦
):
    print("--------------------------------------------------")
    print(f"ğŸ§ª å…¨æµç¨‹æµ‹è¯• (Full Model + Loss): {name}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        model = model.to(device)
        criterion = criterion.to(device)
        model.train()
        
        # å‡†å¤‡æ•°æ®
        aux = aux_input.to(device, non_blocking=True)
        main = main_input.to(device, non_blocking=True)
        tgt = target.to(device, non_blocking=True)

        opt = torch.optim.AdamW(model.parameters(), lr=lr)
        scaler = torch.amp.GradScaler('cuda', enabled=use_amp)

        print("   ğŸ”¥ é¢„çƒ­ä¸­ (å«åä¼ )...")
        for _ in range(warmup):
            torch.compiler.cudagraph_mark_step_begin()
            opt.zero_grad(set_to_none=True)
            with torch.amp.autocast('cuda', enabled=use_amp):
                # 1. Forward
                pred = model(aux, main)
                # 2. Loss (ä¼ å…¥ main ä½œä¸º input_mosaic_low_res)
                loss = criterion(pred, tgt, input_mosaic_low_res=main)
            # 3. Backward
            scaler.scale(loss).backward()
            scaler.step(opt)
            scaler.update()
        if torch.cuda.is_available(): torch.cuda.synchronize()

        fwd_t = loss_t = bwd_t = step_t = 0.0

        for _ in range(iters):
            torch.compiler.cudagraph_mark_step_begin()
            opt.zero_grad(set_to_none=True)

            # --- Forward ---
            t0 = time.time()
            with torch.amp.autocast('cuda', enabled=use_amp):
                pred = model(aux, main)
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t1 = time.time()

            # --- Loss ---
            with torch.amp.autocast('cuda', enabled=use_amp):
                # HybridLoss éœ€è¦ (pred, target, low_res_input)
                loss = criterion(pred, tgt, input_mosaic_low_res=main)
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t2 = time.time()

            # --- Backward ---
            scaler.scale(loss).backward()
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t3 = time.time()

            # --- Optimizer ---
            scaler.step(opt)
            scaler.update()
            if torch.cuda.is_available(): torch.cuda.synchronize()
            t4 = time.time()

            fwd_t += (t1 - t0)
            loss_t += (t2 - t1)
            bwd_t += (t3 - t2)
            step_t += (t4 - t3)

        n = iters
        fwd_ms = fwd_t / n * 1000.0
        loss_ms = loss_t / n * 1000.0
        bwd_ms = bwd_t / n * 1000.0
        step_ms = step_t / n * 1000.0
        total_ms = fwd_ms + loss_ms + bwd_ms + step_ms

        print(f"   â±ï¸ Model Fwd : {fwd_ms:.2f} ms")
        print(f"   â±ï¸ Loss Calc : {loss_ms:.2f} ms")
        print(f"   â±ï¸ Backward  : {bwd_ms:.2f} ms")
        print(f"   â±ï¸ Opt Step  : {step_ms:.2f} ms")
        print(f"   âœ… Total Time: {total_ms:.2f} ms / iter")

        return {"total_ms": total_ms}

    except Exception as e:
        print(f"   âŒ Full Model æµ‹è¯•å¤±è´¥: {e}")
        return {"total_ms": float("inf")}
    finally:
        _clean(model, criterion, aux_input, main_input, target)


if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"ğŸ”¥ ç¡¬ä»¶: {torch.cuda.get_device_name(0)}")
    
    # =========================
    # æµ‹è¯•å‚æ•°
    # =========================
    B, T, H, W = 24, 3, 120, 120
    DIM = 64
    AUX_C = 9
    MAIN_C = 1
    
    print(f"âš™ï¸ æµ‹è¯•å‚æ•°: Batch={B}, Dim={DIM}, Size={H}x{W}, T={T}")

    # æ„é€ æ•°æ®
    # å•æ¨¡å—ç”¨æ•°æ®
    df = torch.randn(B, DIM, T, H, W)
    da = torch.randn(B, DIM, T, H, W)
    
    # ğŸ”¥ å…¨æ¨¡å‹ç”¨æ•°æ®
    # dra: è¾…åŠ©æ•°æ® (9é€šé“)
    dra = torch.randn(B, AUX_C, T, H, W)
    # dm: ä¸»è¾“å…¥æ•°æ® (1é€šé“, ä¹Ÿæ˜¯ Mosaic ä½æ¸…è¾“å…¥)
    dm = torch.randn(B, MAIN_C, T, H, W)
    # target: ç›®æ ‡æ•°æ® (1é€šé“, é«˜æ¸…çœŸå€¼)
    target = torch.randn(B, MAIN_C, T, H, W)

    results_fwd = {}
    results_step = {}

    # ========== 1. å•æ¨¡å—æµ‹è¯• ==========
    results_fwd["3D Conv"] = benchmark_forward("3Då·ç§¯", MultiScaleBlock3D(channels=DIM), df, iters=50)
    results_step["3D Conv"] = benchmark_trainstep("3Då·ç§¯", MultiScaleBlock3D(channels=DIM), df, iters=10)

    results_fwd["MoE"] = benchmark_forward("MoE", MoEBlock(dim=DIM, num_experts=3, top_k=1), df, iters=50)
    results_step["MoE"] = benchmark_trainstep("MoE", MoEBlock(dim=DIM, num_experts=3, top_k=1), df, iters=10)

    mamba_mod = MambaAdapter(dim=DIM)
    # ç¼–è¯‘ Mamba (å®é™…ä¸Šæ˜¯ disable)
    try:
        mamba_mod = torch.compile(mamba_mod, mode='reduce-overhead')
    except: pass
    
    results_fwd["Mamba"] = benchmark_forward("Mamba", mamba_mod, df, iters=50)
    results_step["Mamba"] = benchmark_trainstep("Mamba", mamba_mod, df, iters=10)

    results_fwd["Fusion"] = benchmark_forward("èåˆå±‚", SFTLayer3D(channels=DIM), (df, da), iters=50)

    # ========== 2. ğŸ”¥ å…¨æ¨¡å‹ + Loss æµ‹è¯• ==========
    print("\n=========================")
    print("ğŸš€ å‡†å¤‡è¿›è¡Œ DSTCarbonFormer å…¨æ¨¡å‹æµ‹è¯•...")
    
    full_model = DSTCarbonFormer(aux_c=AUX_C, main_c=MAIN_C, dim=DIM)
    loss_fn = HybridLoss(consistency_scale=4) # å®ä¾‹åŒ– HybridLoss
    
    benchmark_full_model_trainstep(
        name="DSTCarbonFormer + HybridLoss",
        model=full_model,
        criterion=loss_fn,
        aux_input=dra,
        main_input=dm,
        target=target,
        iters=20,
        use_amp=True
    )

    # æ±‡æ€»è¾“å‡º
    print("\n=========================")
    print("ğŸ“Š Forward-only ç»“æœæ±‡æ€» (ms/batch)")
    for k, v in results_fwd.items():
        print(f"{k:12s}: {v:.2f} ms")

    print("\n=========================")
    print("ğŸ“Š Trainstep ç»“æœæ±‡æ€» (ms/iter)")
    for k, d in results_step.items():
        if "total_ms" in d:
            print(f"{k:12s}: total={d['total_ms']:.2f}")

    print("\nâœ… æµ‹è¯•å®Œæˆã€‚")