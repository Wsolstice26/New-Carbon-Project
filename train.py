import os

# ==========================================
# ğŸ”‡ 1. æ ¸å¿ƒè®¾ç½®ï¼šè®© MIOpen é—­å˜´
# ==========================================
os.environ['MIOPEN_LOG_LEVEL'] = '3'
os.environ['MIOPEN_ENABLE_LOGGING'] = '0'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import time

# å¯¼å…¥ä½ çš„æ¨¡å—
from data.dataset import DualStreamDataset
from models.network import DSTCarbonFormer
from models.losses import HybridLoss
from config import CONFIG

# å®šä¹‰è¿˜åŸçœŸå®å€¼æ‰€éœ€çš„å‚æ•° (å¿…é¡»ä¸ dataset.py ä¸­çš„ NORM_MAIN_LOG ä¿æŒä¸€è‡´)
NORM_FACTOR = 11.0

def train():
    # 1. è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ==========================================
    # ğŸ›‘ 2. ç¨³å®šæ€§è®¾ç½® (é˜²å¡æ­»)
    # ==========================================
    # ç¦æ­¢è¿‡åº¦å¯»ä¼˜ï¼Œé˜²æ­¢ Windows TDR æ€æ­»è¿›ç¨‹
    torch.backends.cudnn.benchmark = False 
    torch.backends.cudnn.deterministic = True
    
    # å¯ç”¨æ··åˆç²¾åº¦ (AMP)
    scaler = torch.amp.GradScaler('cuda')
    print(f"âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (Batch Size = {CONFIG['batch_size']})")

    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # 3. å‡†å¤‡æ•°æ®
    print(f"ğŸ“¦ åŠ è½½æ•°æ®...")
    train_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'train')
    val_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'val')
    
    train_dl = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, 
                          num_workers=CONFIG['num_workers'], pin_memory=True)
    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, 
                        num_workers=CONFIG['num_workers'], pin_memory=True)
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ (å« FFT ç¡¬çº¦æŸå±‚)
    print("ğŸ—ï¸ åˆå§‹åŒ– SEN2SR å¢å¼ºç‰ˆæ¨¡å‹...")
    model = DSTCarbonFormer(aux_c=9, main_c=1).to(device)
    
    # 5. ä¼˜åŒ–å™¨ä¸æŸå¤±
    print("âš–ï¸ ä½¿ç”¨ SEN2SR ç‰©ç†ä¸€è‡´æ€§æŸå¤±å‡½æ•°...")
    criterion = HybridLoss(alpha=1.0, beta=0.1, gamma=0.05, delta=1.0).to(device)
    
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])
    
    best_loss = float('inf')
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ! (SEN2SR Mode) | åå½’ä¸€åŒ–å› å­: {NORM_FACTOR}")
    start_time = time.time()
    
    for epoch in range(1, CONFIG['epochs']+1):
        model.train()
        train_loss = 0
        loop = tqdm(train_dl, desc=f"Ep {epoch}/{CONFIG['epochs']}")
        
        for aux, main, target in loop:
            aux = aux.to(device, non_blocking=True)
            main = main.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            with torch.amp.autocast('cuda'):
                # å‰å‘ä¼ æ’­
                pred = model(aux, main)
                # è®¡ç®—æŸå¤±
                loss = criterion(pred, target, input_main=main)
            
            # åå‘ä¼ æ’­ä¸æ¢¯åº¦è£å‰ª
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()

            # ==========================================
            # ğŸ”¥ å®æ—¶è®¡ç®—å¹¶æ˜¾ç¤ºçœŸå®è¯¯å·® (MAE å¨)
            # ==========================================
            with torch.no_grad():
                # 1. è¿˜åŸåˆ°çœŸå®ç‰©ç†å€¼ (åLog + åå½’ä¸€åŒ–)
                # clamp(min=0) ä¿è¯ä¸å‡ºç°è´Ÿæ•°ç¢³æ’æ”¾
                pred_real = torch.expm1(pred.detach() * NORM_FACTOR).clamp(min=0)
                target_real = torch.expm1(target * NORM_FACTOR).clamp(min=0)
                
                # 2. è®¡ç®—å½“å‰ Batch çš„å¹³å‡è¯¯å·® (å¨)
                batch_mae = torch.abs(pred_real - target_real).mean().item()

            # æ›´æ–°è¿›åº¦æ¡ï¼šåŒæ—¶æ˜¾ç¤º loss å’Œ mae (å¨)
            loop.set_postfix(loss=loss.item(), mae=f"{batch_mae:.1f}")
            
        avg_train_loss = train_loss / len(train_dl)
        
        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0
        total_real_mae = 0 
        
        with torch.no_grad():
            for aux, main, target in val_dl:
                aux = aux.to(device)
                main = main.to(device)
                target = target.to(device)
                
                with torch.amp.autocast('cuda'):
                    pred = model(aux, main)
                    val_loss += criterion(pred, target, input_main=main).item()
                    
                    # éªŒè¯é›†ä¹Ÿè®¡ç®—çœŸå®è¯¯å·®
                    pred_real = torch.expm1(pred * NORM_FACTOR).clamp(min=0)
                    target_real = torch.expm1(target * NORM_FACTOR).clamp(min=0)
                    batch_mae = torch.abs(pred_real - target_real).mean().item()
                    total_real_mae += batch_mae
        
        avg_val_loss = val_loss / len(val_dl)
        avg_real_mae = total_real_mae / len(val_dl)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"   ğŸ“Š Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f} | ğŸŒ Real MAE: {avg_real_mae:.2f} (å¨) | LR: {current_lr:.2e}")
        
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], "best_model.pth"))
            print(f"   ğŸ† æœ€ä½³æ¨¡å‹å·²æ›´æ–° (Loss: {best_loss:.5f})")
            
        if epoch % CONFIG['save_freq'] == 0:
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], f"epoch_{epoch}.pth"))
            
        scheduler.step()

    print(f"\nğŸ è®­ç»ƒç»“æŸï¼æ€»è€—æ—¶: {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")

if __name__ == "__main__":
    train()