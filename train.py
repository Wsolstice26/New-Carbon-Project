import os
import csv
import shutil # ç”¨äºæ¸…ç†æ—§æ—¥å¿—

# ==========================================
# ğŸ›¡ï¸ 1. æ ¸å¿ƒè®¾ç½®ï¼šå®‰å…¨ä¸æ€§èƒ½ä¼˜åŒ–
# ==========================================
os.environ['MIOPEN_DEBUG_CONV_GEMM'] = '1'
os.environ['MIOPEN_LOG_LEVEL'] = '2' 
os.environ['MIOPEN_ENABLE_LOGGING'] = '0'
os.environ['PYTORCH_ALLOC_CONF'] = 'max_split_size_mb:128'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import time
import glob
import numpy as np

from data.dataset import DualStreamDataset
from models.network import DSTCarbonFormer
from models.losses import HybridLoss
from config import CONFIG 

NORM_FACTOR = 11.0

# ==========================================
# ğŸ“Š v1.9 äº”ç»´æŒ‡æ ‡è®¡ç®—å‡½æ•°
# ==========================================
def calc_detailed_metrics(pred_real, target_real, threshold=1e-6):
    abs_diff = torch.abs(pred_real - target_real)
    global_mae = abs_diff.mean().item()
    
    mask_nonzero = target_real > threshold
    mask_zero = ~mask_nonzero
    
    nonzero_mae = abs_diff[mask_nonzero].mean().item() if mask_nonzero.sum() > 0 else 0.0
    zero_mae = abs_diff[mask_zero].mean().item() if mask_zero.sum() > 0 else 0.0
    
    # Top 1% Ext (Threshold > 1830)
    mask_top1 = target_real > 1830
    top1_mae = abs_diff[mask_top1].mean().item() if mask_top1.sum() > 0 else 0.0
    
    balanced_mae = 0.5 * nonzero_mae + 0.5 * zero_mae
    return global_mae, nonzero_mae, zero_mae, balanced_mae, top1_mae

def get_latest_checkpoint(save_dir):
    if not os.path.exists(save_dir): return None
    latest_path = os.path.join(save_dir, "latest.pth")
    if os.path.exists(latest_path): return latest_path
    files = glob.glob(os.path.join(save_dir, "epoch_*.pth"))
    if not files: return None
    return max(files, key=os.path.getmtime)

def train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
    
    scaler = torch.amp.GradScaler('cuda', init_scale=2048.0)
    print(f"âš¡ æ¨¡å¼: v1.9 Paper-Ready Logging (LR & Adaptive Weights)")

    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # ----------------------------------------
    # ğŸ“ CSV æ—¥å¿—åˆå§‹åŒ– (å«æƒé‡å’Œå­¦ä¹ ç‡)
    # ----------------------------------------
    log_file = os.path.join(CONFIG['save_dir'], 'training_log.csv')
    
    # å¦‚æœæ˜¯é‡æ–°å¼€å§‹ï¼Œä¸”æ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œå»ºè®®æ‰‹åŠ¨æ¸…ç†ä¸€ä¸‹ log_fileï¼Œæˆ–è€…è¿™é‡Œä¼šè‡ªåŠ¨è¿½åŠ 
    if not os.path.exists(log_file):
        with open(log_file, 'w', newline='') as f:
            writer = csv.writer(f)
            # å®Œç¾çš„è®ºæ–‡æ•°æ®è¡¨å¤´
            header = [
                'Epoch', 'LR',                 # è®­ç»ƒè¿›åº¦ä¸å­¦ä¹ ç‡
                'Train_Loss', 'Val_Loss',      # Loss æ›²çº¿
                'MAE_Global', 'MAE_Balanced',  # æ ¸å¿ƒæŒ‡æ ‡
                'MAE_City', 'MAE_Bg', 'MAE_Ext', # ç»†èŠ‚æŒ‡æ ‡
                'W_Pixel', 'W_SSIM', 'W_TV'    # ğŸ”¥ è‡ªé€‚åº”æƒé‡å˜åŒ– (è®ºæ–‡æ ¸å¿ƒåˆ›æ–°è¯æ®)
            ]
            writer.writerow(header)
            print(f"ğŸ“ å·²åˆ›å»ºå…¨èƒ½æ—¥å¿—æ–‡ä»¶: {log_file}")

    # ----------------------------------------
    # æ•°æ®åŠ è½½
    # ----------------------------------------
    print(f"ğŸ“¦ åŠ è½½æ•°æ®...")
    train_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'train')
    val_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'val')
    
    # âš ï¸ æ”¹ä¸º False ä»¥ä¿è¯é•¿æ—¶é—´è¿è¡Œçš„ç¨³å®šæ€§
    train_dl = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, 
                          num_workers=CONFIG['num_workers'], pin_memory=True, persistent_workers=True)
    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, 
                        num_workers=CONFIG['num_workers'], pin_memory=True, persistent_workers=True)
    
    model = DSTCarbonFormer(aux_c=9, main_c=1, dim=64).to(device)
    criterion = HybridLoss().to(device)
    
    optimizer = optim.AdamW(list(model.parameters()) + list(criterion.parameters()), lr=CONFIG['lr'], weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)
    
    start_epoch = 1
    best_balanced_mae = float('inf')
    early_stop_counter = 0
    
    # ----------------------------------------
    # æ–­ç‚¹ç»­è®­
    # ----------------------------------------
    if CONFIG['resume']:
        latest_ckpt = get_latest_checkpoint(CONFIG['save_dir'])
        if latest_ckpt:
            print(f"ğŸ”„ æ­£åœ¨æ¢å¤æ£€æŸ¥ç‚¹: {latest_ckpt}")
            try:
                checkpoint = torch.load(latest_ckpt, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                criterion.load_state_dict(checkpoint['criterion_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                
                # å…¼å®¹æ—§ç‰ˆæœ¬çš„ checkpoint (å¦‚æœæ²¡æœ‰ä¿å­˜ scaler)
                if 'scaler_state_dict' in checkpoint:
                    scaler.load_state_dict(checkpoint['scaler_state_dict'])
                
                start_epoch = checkpoint['epoch'] + 1
                best_balanced_mae = checkpoint.get('best_balanced_mae', float('inf')) 
                early_stop_counter = checkpoint.get('early_stop_counter', 0)
                print(f"âœ… æ¢å¤æˆåŠŸ! ä» Epoch {start_epoch} ç»§ç»­")
            except Exception as e:
                print(f"âš ï¸ æ¢å¤å¤±è´¥ ({e})ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (v1.9)...")
    
    for epoch in range(start_epoch, CONFIG['epochs']+1):
        model.train()
        train_loss = 0
        loop = tqdm(train_dl, desc=f"Ep {epoch}/{CONFIG['epochs']}")
        
        for aux, main, target in loop:
            aux, main, target = aux.to(device, non_blocking=True), main.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            with torch.amp.autocast('cuda'):
                pred = model(aux, main)
                loss = criterion(pred.float(), target.float(), input_main=main.float())
            
            if torch.isnan(loss) or torch.isinf(loss):
                continue

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            
            with torch.no_grad():
                pred_real = torch.expm1(pred.float().detach() * NORM_FACTOR).clamp(min=0)
                target_real = torch.expm1(target.float() * NORM_FACTOR).clamp(min=0)
                g_mae, nz_mae, z_mae, bal_mae, ext_mae = calc_detailed_metrics(pred_real, target_real)
            
            loop.set_postfix(L=f"{loss.item():.3f}", G=f"{g_mae:.2f}", C=f"{nz_mae:.2f}", E=f"{ext_mae:.1f}")
            
        avg_train_loss = train_loss / len(train_dl) if len(train_dl) > 0 else 0
        
        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0
        total_metrics = np.zeros(5) 
        batch_count = 0
        
        with torch.no_grad():
            for aux, main, target in val_dl:
                aux, main, target = aux.to(device), main.to(device), target.to(device)
                with torch.amp.autocast('cuda'):
                    pred = model(aux, main)
                    val_loss += criterion(pred.float(), target.float()).item()
                    pred_real = torch.expm1(pred.float() * NORM_FACTOR).clamp(min=0)
                    target_real = torch.expm1(target.float() * NORM_FACTOR).clamp(min=0)
                    m = calc_detailed_metrics(pred_real, target_real)
                    total_metrics += np.array(m)
                    batch_count += 1
        
        avg_val_loss = val_loss / batch_count if batch_count > 0 else 0
        avg_metrics = total_metrics / batch_count if batch_count > 0 else np.zeros(5)
        
        # è·å–å½“å‰çš„å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        
        # è·å–å½“å‰çš„æƒé‡çŠ¶æ€
        weights = torch.exp(criterion.w_params)
        weights = (weights / weights.sum() * 3.0).detach().cpu().numpy()
        w_pixel, w_ssim, w_tv = weights[0], weights[1], weights[2]
        
        print(f"   ğŸ“Š [Val] Bal={avg_metrics[3]:.3f} | ğŸ™ï¸Nz={avg_metrics[1]:.3f} | ğŸ­Ext={avg_metrics[4]:.3f}")
        print(f"   âš–ï¸ [Weights] Px:{w_pixel:.2f} | SSIM:{w_ssim:.2f} | TV:{w_tv:.2f} | LR:{current_lr:.2e}")
        
        # ğŸ”¥ å…¨èƒ½å†™å…¥ CSV
        with open(log_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                epoch, 
                f"{current_lr:.2e}",  # LR
                f"{avg_train_loss:.5f}", 
                f"{avg_val_loss:.5f}", 
                f"{avg_metrics[0]:.4f}", # Global
                f"{avg_metrics[3]:.4f}", # Balanced
                f"{avg_metrics[1]:.4f}", # City
                f"{avg_metrics[2]:.4f}", # Bg
                f"{avg_metrics[4]:.4f}", # Ext
                f"{w_pixel:.4f}",       # W_Pixel
                f"{w_ssim:.4f}",        # W_SSIM
                f"{w_tv:.4f}"           # W_TV
            ])

        # --- ä¿å­˜ä¸æ—©åœ ---
        checkpoint_dict = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'criterion_state_dict': criterion.state_dict(), 
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'scaler_state_dict': scaler.state_dict(),
            'best_balanced_mae': best_balanced_mae,
            'early_stop_counter': early_stop_counter
        }
        torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], "latest.pth"))

        if avg_metrics[3] < best_balanced_mae:
            best_balanced_mae = avg_metrics[3]
            early_stop_counter = 0
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], "best_model.pth"))
            torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], "best_checkpoint.pth"))
            print(f"   ğŸ† New Best! (Balanced: {best_balanced_mae:.4f})")
        else:
            early_stop_counter += 1
            print(f"   â³ Patience ({early_stop_counter}/{CONFIG['patience']})")
        
        if epoch % CONFIG['save_freq'] == 0:
            torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], f"epoch_{epoch}.pth"))
            
        if early_stop_counter >= CONFIG['patience']:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ã€‚")
            break
            
        scheduler.step()

    print(f"\nğŸ è®­ç»ƒç»“æŸï¼")

if __name__ == "__main__":
    train()