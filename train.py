import os

# ==========================================
# ğŸ›¡ï¸ 1. æ ¸å¿ƒè®¾ç½®ï¼šå®‰å…¨æ¨¡å¼ (Safe Mode)
# ==========================================
# å¼ºåˆ¶ä½¿ç”¨ GEMM ç®—æ³• (æœ€ç¨³ï¼Œç»å¯¹ä¸å´©ï¼ŒMoE å¿…å¤‡)
os.environ['MIOPEN_DEBUG_CONV_GEMM'] = '1'
# å±è”½ MIOpen çƒ¦äººçš„è­¦å‘Šæ—¥å¿—
os.environ['MIOPEN_LOG_LEVEL'] = '2' 
os.environ['MIOPEN_ENABLE_LOGGING'] = '0'
# æŒ‡å®šç¼“å­˜è·¯å¾„ï¼Œé˜²æ­¢æƒé™é—®é¢˜
os.environ['MIOPEN_USER_DB_PATH'] = './miopen_cache'
os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import time
import glob
import numpy as np

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from data.dataset import DualStreamDataset
from models.network import DSTCarbonFormer
from models.losses import HybridLoss
from config import CONFIG  # <--- ç›´æ¥ä»æ–‡ä»¶å¯¼å…¥é…ç½®

NORM_FACTOR = 11.0

def get_latest_checkpoint(save_dir):
    """
    ä¼˜å…ˆå¯»æ‰¾ latest.pth (ç”±è„šæœ¬è‡ªåŠ¨æ¯è½®ä¿å­˜)ï¼Œ
    å¦‚æœæ‰¾ä¸åˆ°ï¼Œå†å¯»æ‰¾ epoch_*.pth
    """
    if not os.path.exists(save_dir):
        return None
    
    # 1. ä¼˜å…ˆæ‰¾ latest.pth
    latest_path = os.path.join(save_dir, "latest.pth")
    if os.path.exists(latest_path):
        return latest_path

    # 2. æ‰¾ä¸åˆ°å†æ‰¾å†å²å­˜æ¡£
    files = glob.glob(os.path.join(save_dir, "epoch_*.pth"))
    if not files:
        return None
    return max(files, key=os.path.getmtime)

def train():
    # ----------------------------------------
    # 1. è®¾å¤‡ä¸æ€§èƒ½è®¾ç½®
    # ----------------------------------------
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")
        
        # [MoE å…³é”®è®¾ç½®] å…³é—­ Benchmark é˜²æ­¢åŠ¨æ€å½¢çŠ¶å¯¼è‡´å´©æºƒ
        torch.backends.cudnn.benchmark = False 
        torch.backends.cudnn.deterministic = True
        print("ğŸ›¡ï¸ å®‰å…¨æ¨¡å¼å·²å¯åŠ¨: Benchmark=False, GEMM=ON")
    
    scaler = torch.amp.GradScaler('cuda')
    print(f"âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # ----------------------------------------
    # 2. æ•°æ®å‡†å¤‡
    # ----------------------------------------
    print(f"ğŸ“¦ åŠ è½½æ•°æ® (Batch Size: {CONFIG['batch_size']})...")
    
    # åŒé‡æ£€æŸ¥è·¯å¾„
    if not os.path.exists(CONFIG['data_dir']):
        print(f"âŒ é”™è¯¯: æ•°æ®è·¯å¾„ä¸å­˜åœ¨ -> {CONFIG['data_dir']}")
        print("è¯·æ£€æŸ¥ config.py ä¸­çš„è·¯å¾„è®¾ç½®ï¼")
        return

    train_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'train')
    val_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'val')
    
    train_dl = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, 
                          num_workers=CONFIG['num_workers'], pin_memory=True, persistent_workers=True)
    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, 
                        num_workers=CONFIG['num_workers'], pin_memory=True, persistent_workers=True)
    
    # ----------------------------------------
    # 3. æ¨¡å‹ä¸ä¼˜åŒ–å™¨
    # ----------------------------------------
    print("ğŸ—ï¸ åˆå§‹åŒ– DSTCarbonFormer æ¨¡å‹ (v1.3)...")
    model = DSTCarbonFormer(aux_c=9, main_c=1, dim=64).to(device)
    
    print("âš–ï¸ åˆå§‹åŒ–æŸå¤±å‡½æ•°...")
    criterion = HybridLoss(alpha=1.0, beta=0.1, gamma=0.1, delta=0.05, eta=0.1).to(device)
    
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)
    
    # ----------------------------------------
    # 4. æ–­ç‚¹ç»­è®­ (Resume)
    # ----------------------------------------
    start_epoch = 1
    best_loss = float('inf')
    early_stop_counter = 0 
    
    if CONFIG['resume']:
        latest_ckpt = get_latest_checkpoint(CONFIG['save_dir'])
        if latest_ckpt:
            print(f"ğŸ”„ å‘ç°æ£€æŸ¥ç‚¹: {latest_ckpt}ï¼Œæ­£åœ¨æ¢å¤...")
            try:
                checkpoint = torch.load(latest_ckpt, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                start_epoch = checkpoint['epoch'] + 1
                best_loss = checkpoint.get('best_loss', float('inf'))
                early_stop_counter = checkpoint.get('early_stop_counter', 0)
                print(f"âœ… æ¢å¤æˆåŠŸ! ä» Epoch {start_epoch} ç»§ç»­")
            except Exception as e:
                print(f"âš ï¸ æ¢å¤å¤±è´¥ ({e})ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹ã€‚")
    
    # ----------------------------------------
    # 5. è®­ç»ƒä¸»å¾ªç¯
    # ----------------------------------------
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ | æ€»è½®æ•°: {CONFIG['epochs']} | æ—©åœè€å¿ƒ: {CONFIG['patience']}")
    total_start = time.time()
    
    for epoch in range(start_epoch, CONFIG['epochs']+1):
        model.train()
        train_loss = 0
        loop = tqdm(train_dl, desc=f"Ep {epoch}/{CONFIG['epochs']}")
        
        for aux, main, target in loop:
            aux = aux.to(device, non_blocking=True)
            main = main.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            try:
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with torch.amp.autocast('cuda'):
                    pred = model(aux, main)
                    loss = criterion(pred, target, input_main=main)
                
                if torch.isnan(loss):
                    print(f"âš ï¸ è­¦å‘Š: Epoch {epoch} å‡ºç° NaN Lossï¼Œè·³è¿‡æ­¤ Batch")
                    continue

                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                
                train_loss += loss.item()
                
                with torch.no_grad():
                    pred_real = torch.expm1(pred.detach() * NORM_FACTOR).clamp(min=0)
                    target_real = torch.expm1(target * NORM_FACTOR).clamp(min=0)
                    batch_mae = torch.abs(pred_real - target_real).mean().item()

                loop.set_postfix(loss=f"{loss.item():.4f}", mae=f"{batch_mae:.2f}")

            except RuntimeError as e:
                if "invalid configuration" in str(e) or "HIP error" in str(e):
                    print(f"\nâŒ ä¸¥é‡é”™è¯¯: æ˜¾å¡é©±åŠ¨å¼‚å¸¸ã€‚å»ºè®®åˆ é™¤ miopen_cache å¹¶é‡å¯ã€‚")
                    raise e
                else:
                    raise e
            
        avg_train_loss = train_loss / len(train_dl) if len(train_dl) > 0 else 0
        
        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0
        total_real_mae = 0 
        
        with torch.no_grad():
            for aux, main, target in val_dl:
                aux = aux.to(device)
                main = main.to(device)
                target = target.to(device)
                
                with torch.amp.autocast('cuda'):
                    pred = model(aux, main)
                    val_loss += criterion(pred, target, input_main=main).item()
                    
                    pred_real = torch.expm1(pred * NORM_FACTOR).clamp(min=0)
                    target_real = torch.expm1(target * NORM_FACTOR).clamp(min=0)
                    total_real_mae += torch.abs(pred_real - target_real).mean().item()
        
        avg_val_loss = val_loss / len(val_dl)
        avg_real_mae = total_real_mae / len(val_dl)
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"   ğŸ“Š Summary: Train_Loss={avg_train_loss:.5f} | Val_Loss={avg_val_loss:.5f} | ğŸŒ MAE={avg_real_mae:.3f} | LR={current_lr:.2e}")
        
        checkpoint_dict = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_loss': best_loss,
            'early_stop_counter': early_stop_counter
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ (ç”¨äº resume)
        torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], "latest.pth"))

        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            early_stop_counter = 0
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], "best_model.pth"))
            torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], "best_checkpoint.pth"))
            print(f"   ğŸ† æœ€ä½³æ¨¡å‹å·²æ›´æ–°!")
        else:
            early_stop_counter += 1
            print(f"   â³ Loss æœªä¸‹é™ ({early_stop_counter}/{CONFIG['patience']})")
        
        # å®šæœŸä¿å­˜å†å²å­˜æ¡£
        if epoch % CONFIG['save_freq'] == 0:
            torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], f"epoch_{epoch}.pth"))
            
        if early_stop_counter >= CONFIG['patience']:
            print(f"\nğŸ›‘ æ—©åœã€‚")
            break
            
        scheduler.step()

    print(f"\nğŸ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶: {(time.time()-total_start)/60:.2f} åˆ†é’Ÿ")

if __name__ == "__main__":
    try:
        train()
    except Exception as e:
        print(f"\nğŸ’¥ è®­ç»ƒå´©æºƒ: {e}")