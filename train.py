import os

# ==========================================
# üöÄ [ÁéØÂ¢ÉË°•‰∏Å] AMD ROCm ÁºìÂ≠ò‰∏é‰ºòÂåñ
# ==========================================
# 1. ËÆæÁΩÆÊåÅ‰πÖÂåñÁºìÂ≠òÁõÆÂΩï (Âä†ÈÄü‰∫åÊ¨°ÂêØÂä®)
cache_dir = os.path.expanduser("~/.cache/miopen")
os.makedirs(cache_dir, exist_ok=True)
os.environ['MIOPEN_USER_DB_PATH'] = cache_dir
os.environ['MIOPEN_CUSTOM_CACHE_DIR'] = cache_dir

# 2. Âº∫Âà∂ÂºÄÂêØ Workspace (Èò≤Ê≠¢ÊòæÂ≠òË≠¶Âëä)
os.environ['MIOPEN_FORCE_USE_WORKSPACE'] = '1'

# 3. Êó•Âøó‰∏éÁ∫øÁ®ã‰ºòÂåñ
os.environ['MIOPEN_LOG_LEVEL'] = '4'
os.environ['MIOPEN_DEBUG_CONV_GEMM'] = '0'
os.environ['MKL_THREADING_LAYER'] = 'GNU'

# ‚ùå [Â∑≤ÁßªÈô§] ÊòæÂ≠òÈîÅ
# ÂàöÊâçÁöÑÊµãËØïËØÅÊòéËøôË°å‰ª£Á†Å‰ºöÂØºËá¥ÊòæÂ≠òÂàÜÈÖçÂ§±Ë¥•(OOM)ÔºåËÆ© PyTorch Ëá™Âä®ÁÆ°ÁêÜÊõ¥ÂÆâÂÖ®
# os.environ['PYTORCH_ALLOC_CONF'] = 'max_split_size_mb:128'

import csv
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import glob
import numpy as np

# ÂØºÂÖ•È°πÁõÆÊ®°Âùó
from data.dataset import DualStreamDataset
from models.network import DSTCarbonFormer
from models.losses import HybridLoss 
from config import CONFIG 

# ÂºÄÂêØ cudnn/miopen Ëá™Âä®ÂØª‰ºò
torch.backends.cudnn.benchmark = True

def calc_detailed_metrics(pred_real, target_real, threshold=1e-6):
    """ËÆ°ÁÆóËØ¶ÁªÜËØÑ‰º∞ÊåáÊ†á"""
    abs_diff = torch.abs(pred_real - target_real)
    global_mae = abs_diff.mean().item()
    
    mask_nonzero = target_real > threshold
    mask_zero = ~mask_nonzero
    
    nonzero_mae = abs_diff[mask_nonzero].mean().item() if mask_nonzero.sum() > 0 else 0.0
    zero_mae = abs_diff[mask_zero].mean().item() if mask_zero.sum() > 0 else 0.0
    
    mask_top1 = target_real > 1830
    top1_mae = abs_diff[mask_top1].mean().item() if mask_top1.sum() > 0 else 0.0
    
    balanced_mae = 0.5 * nonzero_mae + 0.5 * zero_mae
    return global_mae, nonzero_mae, zero_mae, balanced_mae, top1_mae

def get_latest_checkpoint(save_dir):
    if not os.path.exists(save_dir): return None
    latest_path = os.path.join(save_dir, "latest.pth")
    if os.path.exists(latest_path): return latest_path
    files = glob.glob(os.path.join(save_dir, "epoch_*.pth"))
    if not files: return None
    return max(files, key=os.path.getmtime)

def train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üî• ËÆæÂ§á: {device} | Ê®°Âºè: 120x120 Final (Loss Scaled x100)")
    print(f"üìÇ Êï∞ÊçÆÈõÜ: {CONFIG['data_dir']}")
    print(f"üìè Dim: {CONFIG.get('dim', 48)} | Batch: {CONFIG['batch_size']}")
    
    scaler = torch.amp.GradScaler('cuda', init_scale=65535.0)
    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # ÂàùÂßãÂåñÊó•Âøó
    log_file = os.path.join(CONFIG['save_dir'], 'training_log.csv')
    if not os.path.exists(log_file):
        with open(log_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Epoch', 'LR', 'Train_Loss', 'Val_Loss', 
                             'MAE_Global', 'MAE_Balanced', 'MAE_Ext', 
                             'W_Pixel', 'W_SSIM', 'W_TV', 'W_Cons'])

    # --- Âä†ËΩΩÊï∞ÊçÆ ---
    print(f"üì¶ Âä†ËΩΩÊï∞ÊçÆ (Workers={CONFIG['num_workers']})...")
    train_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'train', time_window=CONFIG['time_window'])
    val_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'val', time_window=CONFIG['time_window'])
    
    # Âä®ÊÄÅËÆæÁΩÆ persistent_workers
    use_persistent = (CONFIG['num_workers'] > 0)
    
    train_dl = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, 
                          num_workers=CONFIG['num_workers'], pin_memory=True, 
                          persistent_workers=use_persistent)
                          
    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, 
                        num_workers=CONFIG['num_workers'], pin_memory=True, 
                        persistent_workers=use_persistent)
    
    print(f"‚úÖ Ê†∑Êú¨Êï∞: Train={len(train_ds)} | Val={len(val_ds)}")

    # --- Ê®°Âûã‰∏é Loss ---
    model = DSTCarbonFormer(aux_c=9, main_c=1, dim=CONFIG.get('dim', 48)).to(device)
    criterion = HybridLoss(consistency_scale=CONFIG['consistency_scale']).to(device)
    
    optimizer = optim.AdamW(list(model.parameters()) + list(criterion.parameters()), lr=CONFIG['lr'], weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)
    
    start_epoch = 1
    best_balanced_mae = float('inf')
    early_stop_counter = 0

    # --- ÊÅ¢Â§çÊñ≠ÁÇπ ---
    if CONFIG['resume']:
        latest_ckpt = get_latest_checkpoint(CONFIG['save_dir'])
        if latest_ckpt:
            print(f"üîÑ ÊÅ¢Â§çÊ£ÄÊü•ÁÇπ: {latest_ckpt}")
            try:
                checkpoint = torch.load(latest_ckpt, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                if 'criterion_state_dict' in checkpoint:
                     try: criterion.load_state_dict(checkpoint['criterion_state_dict'])
                     except: print("‚ö†Ô∏è LossÊùÉÈáç‰∏çÂåπÈÖçÔºåÂ∑≤ÈáçÁΩÆ")
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                if 'scaler_state_dict' in checkpoint:
                    scaler.load_state_dict(checkpoint['scaler_state_dict'])
                start_epoch = checkpoint['epoch'] + 1
                best_balanced_mae = checkpoint.get('best_balanced_mae', float('inf'))
                print(f"‚úÖ ÊÅ¢Â§çÊàêÂäü! ‰ªé Ep {start_epoch} ÂºÄÂßã")
            except Exception as e:
                print(f"‚ö†Ô∏è ÊÅ¢Â§çÂ§±Ë¥• ({e})ÔºåÈáçÊñ∞ÂºÄÂßã")

    # --- ËÆ≠ÁªÉÂæ™ÁéØ ---
    print(f"\nüöÄ ÂºÄÂßãËÆ≠ÁªÉ...")
    for epoch in range(start_epoch, CONFIG['epochs']+1):
        model.train()
        train_loss = 0
        loop = tqdm(train_dl, desc=f"Ep {epoch}/{CONFIG['epochs']}")
        
        for aux, main, target in loop:
            aux, main, target = aux.to(device, non_blocking=True), main.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            with torch.amp.autocast('cuda'):
                pred = model(aux, main)
                # üî• [‰øÆÊîπ] Â∞Ü Loss ÊîæÂ§ß 100 ÂÄç
                # ËøôÊ†∑ log ÈáåÁöÑ loss ÂÄº‰ºöÂèòÊàê 0.x Êàñ 1.xÔºåÁúãËµ∑Êù•Êõ¥Áõ¥ËßÇ
                loss = criterion(pred, target, main) * 100.0
            
            if torch.isnan(loss): 
                print("‚ö†Ô∏è Loss is NaN!"); continue

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            
            with torch.no_grad():
                pred_real = torch.expm1(pred.float().detach() * CONFIG['norm_factor']).clamp(min=0)
                target_real = torch.expm1(target.float() * CONFIG['norm_factor']).clamp(min=0)
                _, _, _, b_mae, _ = calc_detailed_metrics(pred_real, target_real)
            
            loop.set_postfix(L=f"{loss.item():.3f}", B=f"{b_mae:.2f}")
            
        avg_train_loss = train_loss / len(train_dl) if len(train_dl) > 0 else 0
        
        # --- È™åËØÅ ---
        model.eval()
        val_loss = 0
        total_metrics = np.zeros(5) 
        batch_count = 0
        
        with torch.no_grad():
            for aux, main, target in val_dl:
                aux, main, target = aux.to(device), main.to(device), target.to(device)
                with torch.amp.autocast('cuda'):
                    pred = model(aux, main)
                    # üî• [‰øÆÊîπ] È™åËØÅÈõÜ Loss ‰πüË¶ÅËÆ∞ÂæóÊîæÂ§ßÔºå‰øùÊåÅ‰∏ÄËá¥
                    val_loss += (criterion(pred, target, main) * 100.0).item()
                    
                    pred_real = torch.expm1(pred.float() * CONFIG['norm_factor']).clamp(min=0)
                    target_real = torch.expm1(target.float() * CONFIG['norm_factor']).clamp(min=0)
                    m = calc_detailed_metrics(pred_real, target_real)
                    total_metrics += np.array(m)
                    batch_count += 1
        
        avg_val_loss = val_loss / batch_count if batch_count > 0 else 0
        avg_metrics = total_metrics / batch_count if batch_count > 0 else np.zeros(5)
        
        lr = optimizer.param_groups[0]['lr']
        ws = torch.exp(criterion.w_params)
        ws = (ws / ws.sum() * 4.0).detach().cpu().numpy()
        
        print(f"   üìä Val Loss={avg_val_loss:.4f} | Bal MAE={avg_metrics[3]:.3f}")
        print(f"   ‚öñÔ∏è Weights -> Px:{ws[0]:.2f} SSIM:{ws[1]:.2f} TV:{ws[2]:.2f} Cons:{ws[3]:.2f}")

        # --- ‰øùÂ≠òËÆ∞ÂΩï ---
        with open(log_file, 'a', newline='') as f:
            csv.writer(f).writerow([
                epoch, f"{lr:.2e}", 
                f"{avg_train_loss:.5f}", f"{avg_val_loss:.5f}", 
                f"{avg_metrics[0]:.4f}", f"{avg_metrics[3]:.4f}", f"{avg_metrics[4]:.4f}", 
                f"{ws[0]:.3f}", f"{ws[1]:.3f}", f"{ws[2]:.3f}", f"{ws[3]:.3f}"
            ])

        ckpt = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'criterion_state_dict': criterion.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_balanced_mae': best_balanced_mae
        }
        torch.save(ckpt, os.path.join(CONFIG['save_dir'], "latest.pth"))

        if avg_metrics[3] < best_balanced_mae:
            best_balanced_mae = avg_metrics[3]
            early_stop_counter = 0
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], "best_model.pth"))
            print(f"   üèÜ New Best Model Saved!")
        else:
            early_stop_counter += 1
            print(f"   ‚è≥ No improve {early_stop_counter}/{CONFIG['patience']}")
            
        if early_stop_counter >= CONFIG['patience']: break
        scheduler.step()

if __name__ == "__main__":
    train()