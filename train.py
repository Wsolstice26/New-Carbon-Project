import os

# ==========================================
# ğŸ›¡ï¸ 1. æ ¸å¿ƒè®¾ç½®ï¼šå®‰å…¨ä¸æ€§èƒ½ä¼˜åŒ–
# ==========================================
# å¼€å¯ MIOpen GEMM ç®—æ³•ä»¥è·å¾— AMD æ˜¾å¡æœ€ä½³å·ç§¯æ€§èƒ½
os.environ['MIOPEN_DEBUG_CONV_GEMM'] = '1'
# ç¦ç”¨ MIOpen è¿‡äºè¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼Œä¿æŒæ§åˆ¶å°æ¸…çˆ½
os.environ['MIOPEN_LOG_LEVEL'] = '2' 
os.environ['MIOPEN_ENABLE_LOGGING'] = '0'
os.environ['MIOPEN_USER_DB_PATH'] = './miopen_cache'

# âœ… [å…³é”®] AMD æ˜¾å¡é˜²æ­¢æ˜¾å­˜ç¢ç‰‡åŒ–è®¾ç½®
# è¿™èƒ½æœ‰æ•ˆè§£å†³é•¿æ—¶é—´è®­ç»ƒåå‡ºç°çš„ "Out of Memory" é”™è¯¯
os.environ['PYTORCH_ALLOC_CONF'] = 'max_split_size_mb:128'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import time
import glob
import numpy as np

# å¯¼å…¥é¡¹ç›®è‡ªå®šä¹‰æ¨¡å—
from data.dataset import DualStreamDataset
from models.network import DSTCarbonFormer
from models.losses import HybridLoss
from config import CONFIG 

# ç¢³æ’æ”¾æ•°æ®çš„ Log å½’ä¸€åŒ–å› å­ (å¯¹åº”åŸå§‹æ•°æ®çš„ max log å€¼)
NORM_FACTOR = 11.0

# ==========================================
# ğŸ“Š [v1.7 å‡çº§] ç²¾ç»†åŒ–æŒ‡æ ‡è®¡ç®—å‡½æ•°
# ==========================================
def calc_detailed_metrics(pred_real, target_real, threshold=1e-6):
    """
    è®¡ç®—å››ä¸ªç»´åº¦çš„ MAE (Mean Absolute Error)ï¼Œå…¨æ–¹ä½è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    
    å‚æ•°:
        pred_real: åå½’ä¸€åŒ–åçš„é¢„æµ‹å€¼ (çœŸå®å¨æ•°)
        target_real: åå½’ä¸€åŒ–åçš„çœŸå®æ ‡ç­¾ (çœŸå®å¨æ•°)
    
    è¿”å›:
        global_mae: å…¨å±€å¹³å‡è¯¯å·®
        nz_mae: åŸå¸‚/é«˜æ’æ”¾åŒºåŸŸè¯¯å·®
        z_mae: èƒŒæ™¯åŒºåŸŸè¯¯å·®
        balanced_mae: å¹³è¡¡åçš„æ ¸å¿ƒæŒ‡æŒ¥æ£’æŒ‡æ ‡
    """
    abs_diff = torch.abs(pred_real - target_real)
    
    # 1. Global MAE (ä¼ ç»ŸæŒ‡æ ‡)
    global_mae = abs_diff.mean().item()
    
    # 2. ç”Ÿæˆæ©ç  (åŒºåˆ†åŸå¸‚å’ŒèƒŒæ™¯)
    mask_nonzero = target_real > threshold
    mask_zero = ~mask_nonzero
    
    # 3. Non-Zero MAE (åŸå¸‚åŒºåŸŸ - æ”»åšé‡ç‚¹)
    if mask_nonzero.sum() > 0:
        nonzero_mae = abs_diff[mask_nonzero].mean().item()
    else:
        nonzero_mae = 0.0
        
    # 4. Zero MAE (èƒŒæ™¯åŒºåŸŸ - ç›‘æ§å™ªç‚¹)
    if mask_zero.sum() > 0:
        zero_mae = abs_diff[mask_zero].mean().item()
    else:
        zero_mae = 0.0
        
    # ğŸ”¥ [v1.7 æ ¸å¿ƒæ”¹è¿›] Balanced MAE
    # å³ä½¿åŸå¸‚åªå  1% çš„é¢ç§¯ï¼Œå®ƒåœ¨è¯„ä»·ä½“ç³»ä¸­ä¹Ÿå¿…é¡»å  50% çš„æƒé‡ã€‚
    # è¿™æ˜¯æ—©åœ (Early Stopping) çš„å”¯ä¸€ä¾æ®ã€‚
    balanced_mae = 0.5 * nonzero_mae + 0.5 * zero_mae
        
    return global_mae, nonzero_mae, zero_mae, balanced_mae

def get_latest_checkpoint(save_dir):
    """
    è‡ªåŠ¨æŸ¥æ‰¾ä¿å­˜ç›®å½•ä¸­æœ€æ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ (.pth)
    """
    if not os.path.exists(save_dir): 
        return None
    
    # ä¼˜å…ˆæŸ¥æ‰¾ latest.pth
    latest_path = os.path.join(save_dir, "latest.pth")
    if os.path.exists(latest_path): 
        return latest_path
        
    # å¦åˆ™æŒ‰æ—¶é—´æˆ³æŸ¥æ‰¾æœ€æ–°çš„ epoch_*.pth
    files = glob.glob(os.path.join(save_dir, "epoch_*.pth"))
    if not files:
        return None
        
    return max(files, key=os.path.getmtime)

def train():
    # ----------------------------------------
    # 1. ç¯å¢ƒä¸è®¾å¤‡åˆå§‹åŒ–
    # ----------------------------------------
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")
        # æ˜¾å¼å…³é—­ Benchmark ä»¥ä¿è¯åœ¨ ROCm ä¸Šçš„ç¨³å®šæ€§
        torch.backends.cudnn.benchmark = False 
        torch.backends.cudnn.deterministic = True
    
    # åˆå§‹åŒ– AMP (æ··åˆç²¾åº¦è®­ç»ƒ)
    # init_scale è®¾ä¸º 2048 å¯ä»¥é˜²æ­¢åˆå§‹æ¢¯åº¦è¿‡å°ä¸‹æº¢
    scaler = torch.amp.GradScaler('cuda', init_scale=2048.0)
    print(f"âš¡ æ¨¡å¼: Smart AMP (Init Scale=2048) + AMD Optimized")

    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # ----------------------------------------
    # 2. æ•°æ®é›†åŠ è½½
    # ----------------------------------------
    print(f"ğŸ“¦ åŠ è½½æ•°æ® (Batch Size: {CONFIG['batch_size']})...")
    # åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'train')
    val_ds = DualStreamDataset(CONFIG['data_dir'], CONFIG['split_config'], 'val')
    
    # è®¾ç½® DataLoader
    # pin_memory=True åŠ é€Ÿ CPU åˆ° GPU çš„æ•°æ®ä¼ è¾“
    # persistent_workers=True é¿å…æ¯ä¸ª Epoch é‡å¯è¿›ç¨‹çš„å¼€é”€
    train_dl = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, 
                          num_workers=CONFIG['num_workers'], pin_memory=True, persistent_workers=True)
    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, 
                        num_workers=CONFIG['num_workers'], pin_memory=True, persistent_workers=True)
    
    # ----------------------------------------
    # 3. æ¨¡å‹æ„å»ºä¸ä¼˜åŒ–å™¨è®¾ç½®
    # ----------------------------------------
    print("ğŸ—ï¸ åˆå§‹åŒ– DSTCarbonFormer æ¨¡å‹ (v1.7 Balanced Edition)...")
    model = DSTCarbonFormer(aux_c=9, main_c=1, dim=64).to(device)
    
    # åˆå§‹åŒ–è‡ªé€‚åº”æ··åˆæŸå¤± (HybridLoss v1.7)
    # å†…éƒ¨å·²åŒ…å« BalancedCharbonnierLoss å’Œå¯å­¦ä¹ æƒé‡
    criterion = HybridLoss().to(device)
    
    # ğŸ”¥ [å…³é”®] å°† Loss çš„å‚æ•° (w_params) ä¹ŸåŠ å…¥ä¼˜åŒ–å™¨
    # è¿™æ · AdamW å°±ä¼šåŒæ—¶ä¼˜åŒ–ç½‘ç»œæƒé‡å’Œ Loss çš„å¹³è¡¡ç³»æ•°
    optimizer = optim.AdamW(
        list(model.parameters()) + list(criterion.parameters()), 
        lr=CONFIG['lr'], 
        weight_decay=1e-4
    )
    
    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)
    
    # è®­ç»ƒçŠ¶æ€å˜é‡åˆå§‹åŒ–
    start_epoch = 1
    best_balanced_mae = float('inf')  # è®°å½•æœ€ä½³å¹³è¡¡æŒ‡æ ‡
    early_stop_counter = 0            # æ—©åœè®¡æ•°å™¨
    
    # ----------------------------------------
    # 4. æ–­ç‚¹ç»­è®­é€»è¾‘ (Resume)
    # ----------------------------------------
    if CONFIG['resume']:
        latest_ckpt = get_latest_checkpoint(CONFIG['save_dir'])
        if latest_ckpt:
            print(f"ğŸ”„ æ­£åœ¨æ¢å¤æ£€æŸ¥ç‚¹: {latest_ckpt}")
            try:
                checkpoint = torch.load(latest_ckpt, map_location=device)
                
                # æ¢å¤æ¨¡å‹å’Œ Loss çŠ¶æ€
                model.load_state_dict(checkpoint['model_state_dict'])
                criterion.load_state_dict(checkpoint['criterion_state_dict']) # æ¢å¤å­¦ä¹ åˆ°çš„æƒé‡
                
                # æ¢å¤ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                
                # æ¢å¤è®­ç»ƒè¿›åº¦
                start_epoch = checkpoint['epoch'] + 1
                best_balanced_mae = checkpoint.get('best_balanced_mae', float('inf')) 
                early_stop_counter = checkpoint.get('early_stop_counter', 0)
                
                if 'scaler_state_dict' in checkpoint:
                    scaler.load_state_dict(checkpoint['scaler_state_dict'])
                    
                print(f"âœ… æ¢å¤æˆåŠŸ! ä» Epoch {start_epoch} ç»§ç»­ (å½“å‰æœ€ä½³å¹³è¡¡MAE: {best_balanced_mae:.4f})")
            except Exception as e:
                print(f"âš ï¸ æ¢å¤å¤±è´¥ ({e})ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    # ----------------------------------------
    # 5. è®­ç»ƒä¸»å¾ªç¯
    # ----------------------------------------
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (v1.7) | ç›®æ ‡: Balanced Loss & Balanced MAE Optimization")
    total_start = time.time()
    
    for epoch in range(start_epoch, CONFIG['epochs']+1):
        model.train()
        train_loss = 0
        loop = tqdm(train_dl, desc=f"Ep {epoch}/{CONFIG['epochs']}")
        
        for aux, main, target in loop:
            # æ•°æ®æ¬è¿åˆ° GPU
            aux = aux.to(device, non_blocking=True)
            main = main.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.amp.autocast('cuda'):
                pred = model(aux, main)
                # è®¡ç®— Loss (è‡ªåŠ¨åº”ç”¨ 50/50 å¹³è¡¡å’Œè‡ªé€‚åº”æƒé‡)
                loss = criterion(pred.float(), target.float(), input_main=main.float())
            
            # NaN ç†”æ–­ä¿æŠ¤
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸ è­¦å‘Š: Epoch {epoch} å‡ºç° NaN/Inf Lossï¼Œè·³è¿‡æ­¤ Batch")
                optimizer.zero_grad()
                continue

            # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            
            # å®æ—¶è®¡ç®—å½“å‰ Batch çš„ MAE (ä»…ä¾›è§‚å¯Ÿ)
            with torch.no_grad():
                # è¿™é‡Œçš„ NORM_FACTOR å¿…é¡»å’Œä¸Šé¢å®šä¹‰çš„ä¿æŒä¸€è‡´ (11.0)
                pred_real = torch.expm1(pred.float().detach() * NORM_FACTOR).clamp(min=0)
                target_real = torch.expm1(target.float() * NORM_FACTOR).clamp(min=0)
                # ç®€å•è®¡ç®—ä¸€ä¸ªå…¨å±€ MAE çœ‹çœ‹å¤§æ¦‚æƒ…å†µ
                batch_mae = torch.abs(pred_real - target_real).mean().item()
            
            # ä¿®æ”¹è¿›åº¦æ¡æ˜¾ç¤ºï¼ŒåŠ ä¸Š mae
            loop.set_postfix(loss=f"{loss.item():.4f}", mae=f"{batch_mae:.2f}")
            
        avg_train_loss = train_loss / len(train_dl) if len(train_dl) > 0 else 0
        
        # ----------------------------------------
        # 6. éªŒè¯é˜¶æ®µ (Validation)
        # ----------------------------------------
        model.eval()
        val_loss = 0
        
        # åˆå§‹åŒ–æŒ‡æ ‡ç´¯åŠ å™¨ [Global, NZ, Z, Balanced]
        total_metrics = np.zeros(4) 
        
        with torch.no_grad():
            for aux, main, target in val_dl:
                aux, main, target = aux.to(device), main.to(device), target.to(device)
                
                with torch.amp.autocast('cuda'):
                    pred = model(aux, main)
                    val_loss += criterion(pred.float(), target.float()).item()
                    
                    # åå½’ä¸€åŒ–ï¼šå°† Log å€¼è¿˜åŸä¸ºçœŸå®ç¢³æ’æ”¾å¨æ•°
                    pred_real = torch.expm1(pred.float() * NORM_FACTOR).clamp(min=0)
                    target_real = torch.expm1(target.float() * NORM_FACTOR).clamp(min=0)
                    
                    # è®¡ç®—ç²¾ç»†åŒ–æŒ‡æ ‡
                    m = calc_detailed_metrics(pred_real, target_real)
                    total_metrics += np.array(m)
        
        # è®¡ç®—éªŒè¯é›†å¹³å‡æŒ‡æ ‡
        avg_val_loss = val_loss / len(val_dl)
        avg_metrics = total_metrics / len(val_dl)
        
        # è·å–å½“å‰å­¦ä¹ åˆ°çš„ Loss æƒé‡ (ç”¨äºç›‘æ§)
        weights = torch.exp(criterion.w_params)
        weights = (weights / weights.sum() * 3.0).detach().cpu().numpy()
        
        # æ‰“å°è¯¦ç»†æˆ˜æŠ¥
        print(f"   ğŸ“Š [Val] Balanced MAE={avg_metrics[3]:.3f} | ğŸ™ï¸City={avg_metrics[1]:.3f} | ğŸŒ²Bg={avg_metrics[2]:.3f} | ğŸŒGlobal={avg_metrics[0]:.3f}")
        print(f"   âš–ï¸ [Weights] Pixel: {weights[0]:.2f} | SSIM: {weights[1]:.2f} | TV: {weights[2]:.2f}")
        
        # ----------------------------------------
        # 7. æ¨¡å‹ä¿å­˜ä¸æ—©åœé€»è¾‘
        # ----------------------------------------
        checkpoint_dict = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'criterion_state_dict': criterion.state_dict(), 
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'scaler_state_dict': scaler.state_dict(),
            'best_balanced_mae': best_balanced_mae,
            'early_stop_counter': early_stop_counter
        }
        # ä¿å­˜æœ€æ–°æ–­ç‚¹
        torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], "latest.pth"))

        # ğŸ”¥ æ—©åœåˆ¤æ–­ï¼šåªçœ‹ Balanced MAE
        if avg_metrics[3] < best_balanced_mae:
            best_balanced_mae = avg_metrics[3]
            early_stop_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], "best_model.pth"))
            torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], "best_checkpoint.pth"))
            
            print(f"   ğŸ† å‘ç°æ›´ä¼˜æ¨¡å‹! (New Best Balanced MAE: {best_balanced_mae:.4f})")
        else:
            early_stop_counter += 1
            print(f"   â³ æŒ‡æ ‡æœªæ”¹å–„ ({early_stop_counter}/{CONFIG['patience']}) | æœ€ä½³: {best_balanced_mae:.4f}")
        
        # å®šæœŸä¿å­˜ (ä½œä¸ºå¤‡ä»½)
        if epoch % CONFIG['save_freq'] == 0:
            torch.save(checkpoint_dict, os.path.join(CONFIG['save_dir'], f"epoch_{epoch}.pth"))
            
        # è§¦å‘æ—©åœ
        if early_stop_counter >= CONFIG['patience']:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ (Patience={CONFIG['patience']})ã€‚è®­ç»ƒç»“æŸã€‚")
            break
            
        scheduler.step()

    print(f"\nğŸ è®­ç»ƒç»“æŸï¼æ€»è€—æ—¶: {(time.time()-total_start)/60:.2f} åˆ†é’Ÿ")

if __name__ == "__main__":
    try:
        train()
    except Exception as e:
        print(f"\nğŸ’¥ è®­ç»ƒå´©æºƒ: {e}")