import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math

# ==========================================
# 1. åŸºç¡€ç»„ä»¶ (SSIM è¾…åŠ©å‡½æ•°)
# ==========================================
def gaussian(window_size, sigma):
    gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
    return gauss/gauss.sum()

def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window

def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2
    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2
    C1 = 0.01**2; C2 = 0.03**2
    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))
    if size_average: return ssim_map.mean()
    else: return ssim_map.mean(1).mean(1).mean(1)

# ==========================================
# 2. æŸå¤±å‡½æ•°ç±»å®šä¹‰
# ==========================================

# ğŸ”¥ æ–°å¢ï¼šTV Loss (å…¨å˜åˆ†æŸå¤±) - ä¸“é—¨è§£å†³"è¾¹ç•Œä¸å¹³æ»‘"
class TVLoss(nn.Module):
    def __init__(self, tv_loss_weight=1):
        super(TVLoss, self).__init__()
        self.tv_loss_weight = tv_loss_weight

    def forward(self, x):
        batch_size = x.size()[0]
        h_x = x.size()[2]
        w_x = x.size()[3]
        count_h = self._tensor_size(x[:, :, 1:, :])
        count_w = self._tensor_size(x[:, :, :, 1:])
        # è®¡ç®—æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„æ¢¯åº¦å·®å¼‚
        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()
        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()
        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size

    def _tensor_size(self, t):
        return t.size()[1] * t.size()[2] * t.size()[3]

class CharbonnierLoss(nn.Module):
    def __init__(self, eps=1e-3):
        super(CharbonnierLoss, self).__init__()
        self.eps = eps
    
    # ä¿®æ”¹ï¼šæ”¯æŒä¼ å…¥ weight_map è¿›è¡ŒåŠ æƒ
    def forward(self, x, y, weight_map=None):
        diff = x - y
        loss = torch.sqrt(diff * diff + self.eps * self.eps)
        if weight_map is not None:
            loss = loss * weight_map # ğŸ”¥ æ ¸å¿ƒï¼šå¯¹é‡ç‚¹åŒºåŸŸåŠ æƒ
        return torch.mean(loss)

class EdgeLoss(nn.Module):
    def __init__(self):
        super(EdgeLoss, self).__init__()
        k = torch.Tensor([[.05, .25, .4, .25, .05]])
        self.kernel = torch.matmul(k.t(), k).unsqueeze(0).repeat(1, 1, 1, 1)
        if torch.cuda.is_available(): self.kernel = self.kernel.cuda()
        self.loss = CharbonnierLoss()

    def conv_gauss(self, img):
        n_channels, _, kw, kh = self.kernel.shape
        img = F.pad(img, (kw//2, kw//2, kh//2, kh//2), mode='replicate')
        return F.conv2d(img, self.kernel, groups=n_channels)

    def laplacian_kernel(self, current):
        filtered = self.conv_gauss(current)
        down = filtered[:, :, ::2, ::2]
        new_filter = torch.zeros_like(filtered)
        new_filter[:, :, ::2, ::2] = down*4
        filtered = self.conv_gauss(new_filter)
        diff = current - filtered
        return diff

    def forward(self, x, y):
        if x.dim() == 5:
            b, c, t, h, w = x.size()
            x = x.view(b * t, c, h, w)
            y = y.view(b * t, c, h, w)
        return self.loss(self.laplacian_kernel(x), self.laplacian_kernel(y))

class SSIMLoss(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIMLoss, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        if img1.dim() == 5:
            b, c, t, h, w = img1.size()
            img1 = img1.view(b * t, c, h, w)
            img2 = img2.view(b * t, c, h, w)

        (_, channel, _, _) = img1.size()
        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)
            if img1.is_cuda: window = window.cuda(img1.get_device())
            window = window.type_as(img1)
            self.window = window
            self.channel = channel
            
        ssim_val = _ssim(img1, img2, window, self.window_size, channel, self.size_average)
        return F.relu(1 - ssim_val)

class ConsistencyLoss(nn.Module):
    def __init__(self, norm_factor=11.0):
        super().__init__()
        self.loss = nn.L1Loss()
        self.norm_factor = norm_factor 
        
    def forward(self, pred_high_res, input_low_res):
        pred_real = torch.expm1(pred_high_res * self.norm_factor)
        input_real = torch.expm1(input_low_res * self.norm_factor)
        
        target_h, target_w = input_low_res.shape[-2:]
        t_dim = input_low_res.shape[2]
        pred_down_real = F.adaptive_avg_pool3d(pred_real, output_size=(t_dim, target_h, target_w))
        
        pred_down_log = torch.log1p(pred_down_real) / self.norm_factor
        input_log = torch.log1p(input_real) / self.norm_factor 
        
        return self.loss(pred_down_log, input_log)

# ==========================================
# 3. æ··åˆæŸå¤±å‡½æ•° (HybridLoss) - æ ¸å¿ƒä¿®æ”¹ç‰ˆ
# ==========================================

class HybridLoss(nn.Module):
    def __init__(self, alpha=1.0, beta=0.2, gamma=0.2, delta=1.0, eta=0.1):
        super(HybridLoss, self).__init__()
        # æƒé‡é…ç½® (æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œäº†å¾®è°ƒ)
        self.alpha = alpha  # Pixel Loss (å¸¦åŠ æƒ)
        self.beta = beta    # SSIM Loss (è°ƒå¤§ï¼Œå¢å¼ºç»“æ„)
        self.gamma = gamma  # Edge Loss (è°ƒå¤§ï¼Œå¢å¼ºè¾¹ç•Œ)
        self.delta = delta  # Consistency Loss (ç‰©ç†å®ˆæ’)
        self.eta = eta      # ğŸ”¥ æ–°å¢: TV Loss (å¹³æ»‘åº¦)
        
        self.pixel_loss = CharbonnierLoss()
        self.ssim_loss = SSIMLoss()
        self.edge_loss = EdgeLoss()
        self.cons_loss = ConsistencyLoss(norm_factor=11.0)
        self.tv_loss = TVLoss() # åˆå§‹åŒ– TV Loss
        
    def forward(self, pred, target, input_main):
        # 1. æ„å»ºæƒé‡åœ°å›¾ (Weight Map)
        # é€»è¾‘ï¼šå¦‚æœ target > 0 (æœ‰ç¢³æ’æ”¾)ï¼Œæƒé‡è®¾ä¸º 10.0ï¼›å¦åˆ™è®¾ä¸º 1.0
        # æ³¨æ„ï¼štarget å·²ç»è¢« Log+å½’ä¸€åŒ–äº†ï¼Œæ‰€ä»¥ 0 ä¾ç„¶æ˜¯ 0ï¼Œæœ‰å€¼çš„åœ°æ–¹æ˜¯å°æ•°
        # æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªæå°çš„é˜ˆå€¼ 1e-6 æ¥åˆ¤å®šéé›¶åŒºåŸŸ
        
        mask = (target > 1e-6).float()
        # æƒé‡å…¬å¼ï¼šBackground=1.0, Emission=10.0
        weight_map = 1.0 + mask * 9.0 
        
        # 2. è®¡ç®—å„é¡¹æŸå¤±
        l_pix = self.pixel_loss(pred, target, weight_map=weight_map) # ä¼ å…¥æƒé‡
        l_ssim = self.ssim_loss(pred, target)
        l_edge = self.edge_loss(pred, target)
        l_cons = self.cons_loss(pred, input_main)
        
        # å¤„ç† TV Loss (éœ€å…ˆå°† 5D è½¬ 4D: B*T, C, H, W)
        if pred.dim() == 5:
            b, c, t, h, w = pred.size()
            l_tv = self.tv_loss(pred.view(b*t, c, h, w))
        else:
            l_tv = self.tv_loss(pred)
            
        # 3. æ€»å’Œ
        total_loss = (self.alpha * l_pix + 
                      self.beta * l_ssim + 
                      self.gamma * l_edge + 
                      self.delta * l_cons + 
                      self.eta * l_tv)
        
        return total_loss