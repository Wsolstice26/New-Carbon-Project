import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math

# ==========================================
# 1. åŸºç¡€ç»„ä»¶ (SSIM è¾…åŠ©å‡½æ•°)
# ==========================================
def gaussian(window_size, sigma):
    gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
    return gauss/gauss.sum()

def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window

def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2
    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2
    C1 = 0.01**2; C2 = 0.03**2
    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))
    if size_average: return ssim_map.mean()
    else: return ssim_map.mean(1).mean(1).mean(1)

# ==========================================
# 2. æ ¸å¿ƒæŸå¤±æ¨¡å—å®šä¹‰
# ==========================================

class TVLoss(nn.Module):
    """
    å…¨å˜åˆ†æŸå¤± (Total Variation Loss)
    ä½œç”¨ï¼šä¸“é—¨ç”¨äºŽå¹³æ»‘å›¾åƒï¼Œæ¶ˆé™¤è¶…åˆ†è¾¨çŽ‡ä¸­å¸¸è§çš„ç½‘æ ¼ä¼ªå½±å’Œé«˜é¢‘å™ªç‚¹ã€‚
    """
    def __init__(self, tv_loss_weight=1):
        super(TVLoss, self).__init__()
        self.tv_loss_weight = tv_loss_weight

    def forward(self, x):
        batch_size = x.size()[0]
        h_x = x.size()[2]
        w_x = x.size()[3]
        count_h = self._tensor_size(x[:, :, 1:, :])
        count_w = self._tensor_size(x[:, :, :, 1:])
        # è®¡ç®—æ°´å¹³å’Œåž‚ç›´æ–¹å‘çš„æ¢¯åº¦å·®å¼‚
        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()
        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()
        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size

    def _tensor_size(self, t):
        return t.size()[1] * t.size()[2] * t.size()[3]

class BalancedCharbonnierLoss(nn.Module):
    """
    ðŸ”¥ [æ ¸å¿ƒå‡çº§] å¹³è¡¡æŽ©ç  Charbonnier Loss
    ä½œç”¨ï¼š
    1. ä½¿ç”¨ Charbonnier (L1å˜ä½“) ä¿è¯æ•°å€¼å›žå½’çš„é²æ£’æ€§ã€‚
    2. å¼•å…¥å¹³è¡¡æœºåˆ¶ï¼šå¼ºåˆ¶ 'åŸŽå¸‚åŒºåŸŸ' å’Œ 'èƒŒæ™¯åŒºåŸŸ' å¯¹ Loss çš„è´¡çŒ®å„å  50%ã€‚
       è¿™è§£å†³äº†èƒŒæ™¯ 0 å€¼è¿‡å¤šå¯¼è‡´æ¢¯åº¦è¢«ç¨€é‡Šçš„é—®é¢˜ã€‚
    """
    def __init__(self, eps=1e-3):
        super(BalancedCharbonnierLoss, self).__init__()
        self.eps = eps
    
    def forward(self, x, y):
        # 1. è®¡ç®—åŸºç¡€è¯¯å·®å›¾
        diff_sq = (x - y)**2
        loss_map = torch.sqrt(diff_sq + self.eps * self.eps)
        
        # 2. åˆ›å»ºéžé›¶æŽ©ç  (é˜ˆå€¼è®¾ä¸º 1e-6)
        mask = (y > 1e-6).float()
        inv_mask = 1.0 - mask
        
        # 3. åˆ†åˆ«è®¡ç®—åŸŽå¸‚å’ŒèƒŒæ™¯çš„å¹³å‡ Loss
        # åŠ ä¸Š 1e-8 æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º 0 (ä¾‹å¦‚å…¨é»‘å›¾ç‰‡)
        loss_city = (loss_map * mask).sum() / (mask.sum() + 1e-8)
        loss_bg = (loss_map * inv_mask).sum() / (inv_mask.sum() + 1e-8)
        
        # 4. å¼ºåˆ¶ 50/50 å¹³è¡¡
        # æ— è®ºèƒŒæ™¯é¢ç§¯å¤šå¤§ï¼Œå®ƒåªèƒ½è´¡çŒ®ä¸€åŠçš„ Loss
        return 0.5 * loss_city + 0.5 * loss_bg

class SSIMLoss(torch.nn.Module):
    """
    ç»“æž„ç›¸ä¼¼æ€§æŸå¤± (SSIM)
    ä½œç”¨ï¼šä¿è¯é‡å»ºç»“æžœåœ¨è§†è§‰ç»“æž„ï¼ˆè·¯ç½‘ã€çº¹ç†ï¼‰ä¸Šä¸ŽçœŸå€¼ä¸€è‡´ã€‚
    """
    def __init__(self, window_size=11, size_average=True):
        super(SSIMLoss, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        # é€‚é… 5D è¾“å…¥ (B, C, T, H, W) -> å±•å¹³ä¸º 4D è¿›è¡Œå·ç§¯è®¡ç®—
        if img1.dim() == 5:
            b, c, t, h, w = img1.size()
            img1 = img1.view(b * t, c, h, w)
            img2 = img2.view(b * t, c, h, w)

        (_, channel, _, _) = img1.size()
        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)
            if img1.is_cuda: window = window.cuda(img1.get_device())
            window = window.type_as(img1)
            self.window = window
            self.channel = channel
            
        ssim_val = _ssim(img1, img2, window, self.window_size, channel, self.size_average)
        return F.relu(1 - ssim_val)

# ==========================================
# 3. è‡ªé€‚åº”æ··åˆæŸå¤± (æ­£æ•°æƒé‡ç‰ˆ)
# ==========================================
class HybridLoss(nn.Module):
    def __init__(self):
        super(HybridLoss, self).__init__()
        
        # åˆå§‹åŒ–ä¸‰ä¸ªæ ¸å¿ƒ Loss
        self.pixel_loss = BalancedCharbonnierLoss() # å‡çº§ä¸ºå¹³è¡¡ç‰ˆ
        self.ssim_loss = SSIMLoss()
        self.tv_loss = TVLoss()
        
        # ðŸ”¥ [å…³é”®ä¿®æ”¹] å®šä¹‰ 3 ä¸ªå¯å­¦ä¹ çš„æƒé‡å‚æ•°
        # åˆå§‹åŒ–ä¸º 0.0ï¼Œè¿™æ„å‘³ç€åˆå§‹æ—¶åˆ» exp(0)=1ï¼Œå³ä¸‰è€…æƒé‡ç›¸ç­‰
        self.w_params = nn.Parameter(torch.zeros(3))

        # ðŸ”¥ æ–°å¢žï¼šå®šä¹‰æ”¾å¤§å€æ•° (Scale Factor)
        # å»ºè®®è®¾ä¸º 100 æˆ– 1000ï¼Œè®© Loss å›žåˆ° 0.x ~ 1.x çš„åŒºé—´
        self.loss_scale = 1000.0

    def forward(self, pred, target, input_main=None):
        # 1. è®¡ç®—å„åˆ†é¡¹ Loss
        l_pix = self.pixel_loss(pred, target)
        l_ssim = self.ssim_loss(pred, target)
        
        # TV Loss éœ€è¦å¤„ç† 5D æ•°æ®çš„ reshape
        if pred.dim() == 5:
            b, c, t, h, w = pred.size()
            l_tv = self.tv_loss(pred.view(b*t, c, h, w))
        else:
            l_tv = self.tv_loss(pred)
            
        # 2. ðŸ”¥ æƒé‡è‡ªé€‚åº”è®¡ç®— (Softmax å½’ä¸€åŒ–æ€æƒ³)
        # ä½¿ç”¨ exp ç¡®ä¿æƒé‡æ°¸è¿œä¸ºæ­£æ•°ï¼Œé¿å…å‡ºçŽ°è´Ÿæ•° Loss
        weights = torch.exp(self.w_params) 
        
        # å½’ä¸€åŒ–ï¼šè®©æƒé‡ä¹‹å’Œæ’ç­‰äºŽ 3.0
        # è¿™æ ·æ—¢èƒ½ä¿æŒé‡çº§ç¨³å®šï¼Œåˆèƒ½è®©æ¨¡åž‹åŠ¨æ€è°ƒæ•´ä¸‰è€…çš„æ¯”ä¾‹
        weights = weights / weights.sum() * 3.0
        
        # 3. åŠ æƒæ±‚å’Œ
        # weights[0] -> Pixel Loss (æ•°å€¼ç²¾åº¦)
        # weights[1] -> SSIM Loss (ç»“æž„çº¹ç†)
        # weights[2] -> TV Loss (åŽ»ç½‘æ ¼åŒ–)
        total_loss = (weights[0] * l_pix + 
                      weights[1] * l_ssim + 
                      weights[2] * l_tv)
        
        return total_loss * self.loss_scale