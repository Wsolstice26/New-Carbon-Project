# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================
# Loss A: Weighted Linear L1 Loss (Pixel-wise Sum Reduction)
# ============================================================
class WeightedL1Loss(nn.Module):
    """
    [çº¿æ€§ä¸€è‡´æ€§æŸå¤± - åƒç´ çº§æ±‚å’Œç‰ˆ] 
    ç›´æŽ¥ä¼˜åŒ– |pred - gt|ã€‚
    
    å…³é”®å˜åŒ–ï¼š
    ä½¿ç”¨ Sum Reduction (é™¤ä»¥ BatchSize) æ›¿ä»£ Mean Reductionã€‚
    è¿™æ„å‘³ç€ï¼šæ¨¡åž‹ä¸å†é€šè¿‡"å¹³å‡"æ¥ç¨€é‡Šè¯¯å·®ï¼Œè€Œæ˜¯å¿…é¡»é¢å¯¹æ¯ä¸€ä¸ªåƒç´ çš„ç»å¯¹è¯¯å·®æ€»å’Œã€‚
    è¿™å¯¹äºŽç¨€ç–çš„é«˜æŽ’æ”¾ç‚¹ï¼ˆHigh Valuesï¼‰è‡³å…³é‡è¦ã€‚
    """
    def __init__(self, use_charbonnier: bool = False, eps: float = 1e-6):
        super().__init__()
        self.use_charbonnier = bool(use_charbonnier)
        self.eps = float(eps)

    def forward(self, pred: torch.Tensor, gt: torch.Tensor, nz_ratio: torch.Tensor, cv: torch.Tensor) -> torch.Tensor:
        # pred/gt: [B,1,T,12,12] (ç‰©ç†æ•°å€¼)
        
        # 1. è®¡ç®—çº¿æ€§å·®å€¼
        diff = pred - gt
        
        if self.use_charbonnier:
            # Charbonnier Loss (L1 çš„å¹³æ»‘ç‰ˆ)
            loss_map = torch.sqrt(diff * diff + self.eps * self.eps)
        else:
            # æ ‡å‡† L1
            loss_map = diff.abs()
            
        # 2. åŠ¨æ€æƒé‡è®¡ç®— (ä¿æŒåŽŸé€»è¾‘ï¼Œè¿™å¯¹äºŽå¤„ç†é•¿å°¾åˆ†å¸ƒå¾ˆé‡è¦)
        mask_nz = gt > 0
        weights = torch.ones_like(gt)
        
        if mask_nz.any():
            # A. å…¨å±€æƒé‡: log(nz_ratio)
            if isinstance(nz_ratio, torch.Tensor) and nz_ratio.ndim == 1:
                w_global = torch.log(nz_ratio.view(-1, 1, 1, 1, 1) + 1e-6)
                w_global = w_global.expand_as(gt)
            else:
                w_global = torch.log(nz_ratio + 1e-6)
            
            # B. å±€éƒ¨æƒé‡: (1 + log1p(GT)) * CV
            if isinstance(cv, torch.Tensor) and cv.ndim == 1:
                cv_val = cv.view(-1, 1, 1, 1, 1).expand_as(gt)
            else:
                cv_val = cv

            w_local = (1.0 + torch.log1p(gt)) * cv_val
            
            # æˆªæ–­æƒé‡é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ (1.0 ~ 20.0)
            w_final_nz = (w_global * w_local).clamp(min=1.0, max=20.0) 
            weights[mask_nz] = w_final_nz[mask_nz]

        # ðŸš€ã€æ ¸å¿ƒä¿®æ”¹ã€‘Mean -> Sum
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ€»è¯¯å·®ï¼Œç„¶åŽå¯¹ Batch å–å¹³å‡ã€‚
        # è¿™æ ·æ—¢ä¿ç•™äº† Sum çš„å¼ºæ¢¯åº¦ç‰¹æ€§ï¼Œåˆé˜²æ­¢ Batch Size å˜åŒ–å½±å“ Loss è§„æ¨¡ã€‚
        return (loss_map * weights.detach()).sum() / pred.size(0)


# ============================================================
# Loss B: Sparsity prior (Sum Reduction)
# ============================================================
class SparsityLoss(nn.Module):
    """
    [ç¨€ç–æŸå¤±] çº¦æŸ 100m ç»†èŠ‚ï¼Œé˜²æ­¢åº•å™ªã€‚
    """
    def __init__(self):
        super().__init__()

    def forward(self, pred: torch.Tensor) -> torch.Tensor:
        # ðŸš€ã€æ ¸å¿ƒä¿®æ”¹ã€‘Mean -> Sum
        # ä¿æŒä¸Žä¸» Loss é‡çº§ä¸€è‡´
        return pred.abs().sum() / pred.size(0)


# ============================================================
# Loss C: Block Entropy Loss
# ============================================================
class BlockEntropyLoss(nn.Module):
    """
    [ç†µæŸå¤±] çº¦æŸ 100m çº¹ç†ï¼Œé˜²æ­¢æ–¹å—æ•ˆåº”ã€‚
    (æ³¨æ„ï¼šåœ¨ Sum æ¨¡å¼ä¸‹ï¼ŒEntropy çš„æ•°å€¼ç›¸å¯¹è¾ƒå°ï¼Œéœ€è¦åœ¨ Config é‡Œè°ƒå¤§æƒé‡æˆ–è€…ç›´æŽ¥å¿½ç•¥)
    """
    def __init__(
        self,
        scale: int = 10,
        mode: str = "max",
        target_entropy: float = 1.5,
        eps: float = 1e-8,
        soft_valid_k: float = 20.0,
    ):
        super().__init__()
        self.scale = int(scale)
        self.mode = str(mode)
        self.target_entropy = float(target_entropy)
        self.eps = float(eps)
        self.soft_valid_k = float(soft_valid_k)

    def forward(self, pred: torch.Tensor) -> torch.Tensor:
        x = pred.clamp(min=0.0)
        B, C, T, H, W = x.shape
        s = self.scale
        
        if H % s != 0 or W % s != 0:
            return torch.tensor(0.0, device=x.device, requires_grad=True)

        h_grid, w_grid = H // s, W // s
        n = s * s

        blocks = (
            x.view(B, C, T, h_grid, s, w_grid, s)
             .permute(0, 1, 2, 3, 5, 4, 6)
             .reshape(B, C, T, h_grid, w_grid, n)
        )

        block_sum = blocks.sum(dim=-1, keepdim=True)
        p = blocks / (block_sum + self.eps)
        entropy = -(p * torch.log(p + self.eps)).sum(dim=-1)

        soft_valid = torch.sigmoid(self.soft_valid_k * (block_sum.squeeze(-1) - self.eps)).to(entropy.dtype)
        denom = soft_valid.sum().clamp(min=1.0)
        entropy_mean = (entropy * soft_valid).sum() / denom

        if self.mode == "max":
            return -entropy_mean
        return torch.abs(entropy_mean - self.target_entropy)


# ============================================================
# Criterion: HybridLoss (Targeting High R2)
# ============================================================
class HybridLoss(nn.Module):
    def __init__(
        self,
        consistency_scale: int = 10,
        w_sparse: float = 1e-3,
        w_ent: float = 1e-3,
        ent_mode: str = "max",          
        target_entropy: float = 1.5,    
        use_charbonnier_A: bool = False,
    ):
        super().__init__()
        self.w_sparse = float(w_sparse)
        self.w_ent = float(w_ent)

        # 1. ä¸» Loss (Linear Sum)
        self.loss_A = WeightedL1Loss(use_charbonnier=use_charbonnier_A)
        
        # 2. è¾…åŠ© Loss
        self.loss_B = SparsityLoss()
        self.loss_C = BlockEntropyLoss(
            scale=consistency_scale, 
            mode=ent_mode, 
            target_entropy=target_entropy
        )

    def forward(
        self,
        pred: torch.Tensor,               
        target: torch.Tensor,             
        pred_100m: torch.Tensor = None,   
        nz_ratio_win: torch.Tensor = None,
        cv_log_win: torch.Tensor = None
    ) -> torch.Tensor:
        
        # 1. ä¸» Loss (Sum Reduction)
        lA = self.loss_A(pred, target, nz_ratio=nz_ratio_win, cv=cv_log_win)

        # 2. è¾…åŠ© Loss
        p_for_prior = pred_100m if pred_100m is not None else pred

        lB = torch.zeros((), device=pred.device)
        if self.w_sparse > 0:
            lB = self.loss_B(p_for_prior)
            
        lC = torch.zeros((), device=pred.device)
        if self.w_ent > 0:
            lC = self.loss_C(p_for_prior)

        # ç›´æŽ¥æ±‚å’Œ
        total = lA + (self.w_sparse * lB) + (self.w_ent * lC)

        self.last_losses = {
            "lA_linear_sum": lA.detach(),
            "lB_sparse": lB.detach(),
            "lC_ent": lC.detach(),
        }

        return total