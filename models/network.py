# -*- coding: utf-8 -*-
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

from .blocks import (
    MultiScaleBlock3D,
    SFTLayer3D,
    EfficientContextBlock,
    MoEBlock,
    DropPath,           
    TemporalDWConv3d,   
    GatedFusion,
    BiMambaBlock        
)

class DepthwiseSeparableConv3d(nn.Module):
    """æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œæ ‡å‡†ç»“æ„"""
    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):
        super().__init__()
        self.dw = nn.Conv3d(
            in_ch, in_ch, kernel_size=kernel_size, padding=padding, groups=in_ch, bias=False
        )
        self.pw = nn.Conv3d(in_ch, out_ch, kernel_size=1, bias=False)
        self.act = nn.GELU()

    def forward(self, x):
        return self.act(self.pw(self.dw(x)))

class EnhancedResBlock(nn.Module):
    """
    [New] å¢å¼ºå‹æ®‹å·®å—:
    1. Temporal DWConv (å…ˆçœ‹æ—¶é—´)
    2. Spatial DWConv (å†çœ‹ç©ºé—´)
    3. DropPath (éšæœºæ·±åº¦æ­£åˆ™åŒ–)
    """
    def __init__(self, dim, drop_path=0.):
        super().__init__()
        self.temporal_conv = TemporalDWConv3d(dim) # æ—¶åºæ··åˆ
        self.spatial_conv = nn.Sequential(         # ç©ºé—´æ··åˆ
            DepthwiseSeparableConv3d(dim, dim),
            DepthwiseSeparableConv3d(dim, dim)
        )
        # å¦‚æœ drop_path > 0 åˆ™åº”ç”¨ï¼Œå¦åˆ™æ˜¯ Identity
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        # å¼ºåˆ¶æ¨¡å‹å…ˆèåˆå‰åå¸§ä¿¡æ¯
        t_feat = self.temporal_conv(x)
        # æå–ç©ºé—´ç‰¹å¾
        s_feat = self.spatial_conv(t_feat)
        # æ®‹å·®è¿æ¥ + DropPath
        return x + self.drop_path(s_feat)

class DSTCarbonFormer(nn.Module):
    def __init__(self, aux_c=9, main_c=1, 
                 dim=96,               
                 norm_const=11.0, 
                 num_mamba_layers=2,   # [Config] å»ºè®®æ”¹ä¸º2ï¼Œå› ä¸ºç°åœ¨æœ‰3ä¸ªStage
                 num_res_blocks=4,     
                 drop_path_rate=0.1    
                 ):
        super().__init__()
        self.norm_const = float(norm_const)
        self.dim = dim

        # ===========================
        # 1. Heads (ç‰¹å¾ç¼–ç ) - ä¿æŒä¸å˜
        # ===========================
        self.aux_head = nn.Sequential(
            nn.Conv3d(aux_c, dim, 3, padding=1),
            nn.GELU(),
            nn.Conv3d(dim, dim, 3, padding=1),
        )
        self.aux_multiscale = MultiScaleBlock3D(dim)
        
        self.main_head = nn.Sequential(
            nn.Conv3d(main_c, dim, 3, padding=1),
            nn.GELU(),
            nn.Conv3d(dim, dim, 3, padding=1),
        )

        # ===========================
        # 2. Fusion - ä¿æŒä¸å˜
        # ===========================
        self.sft1 = SFTLayer3D(dim)
        self.gated_fusion = GatedFusion(dim) 

        # ===========================
        # 3. Deep Body (æ·±å±‚ç‰¹å¾ä½“) - å¾®è°ƒ RIR
        # ===========================
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, num_res_blocks)]
        
        self.res_blocks = nn.ModuleList([
            EnhancedResBlock(dim, drop_path=dpr[i])
            for i in range(num_res_blocks)
        ])
        
        # [New] RIR (Residual in Residual) æ”¶å°¾å·ç§¯
        # ç”¨äºåœ¨è¿›å…¥ Mamba å‰æ•´ç†ç‰¹å¾
        self.body_tail = nn.Conv3d(dim, dim, 3, 1, 1)

        self.sft2 = SFTLayer3D(dim)
        self.moe_block = MoEBlock(dim)
        self.global_context = EfficientContextBlock(dim)

        # ===========================
        # 4. Hierarchical Bi-Mamba Bottleneck (3-Level U-Net) - æ ¸å¿ƒé‡æ„
        # ===========================
        # ç»“æ„: 120 -> 60 -> 30 -> 15(Global) -> 30 -> 60 -> 120
        
        mamba_dim = 64 

        # --- Level 1 Down: 120 -> 60 ---
        self.down1 = nn.Sequential(
            nn.Conv3d(dim, mamba_dim, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )
        self.mamba_stage1 = nn.ModuleList([
            nn.Sequential(
                nn.LayerNorm(mamba_dim), 
                BiMambaBlock(dim=mamba_dim, d_state=16, d_conv=4, expand=2)
            ) for _ in range(num_mamba_layers)
        ])

        # --- Level 2 Down: 60 -> 30 ---
        self.down2 = nn.Sequential(
            nn.Conv3d(mamba_dim, mamba_dim, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )
        self.mamba_stage2 = nn.ModuleList([
            nn.Sequential(
                nn.LayerNorm(mamba_dim), 
                BiMambaBlock(dim=mamba_dim, d_state=16, d_conv=4, expand=2)
            ) for _ in range(num_mamba_layers)
        ])

        # --- Level 3 Down: 30 -> 15 (ä¸Šå¸è§†è§’) ---
        self.down3 = nn.Sequential(
            nn.Conv3d(mamba_dim, mamba_dim, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )
        self.mamba_stage3 = nn.ModuleList([
            nn.Sequential(
                nn.LayerNorm(mamba_dim), 
                BiMambaBlock(dim=mamba_dim, d_state=16, d_conv=4, expand=2)
            ) for _ in range(num_mamba_layers)
        ])

        # --- Level 3 Up: 15 -> 30 (å¯å­¦ä¹ ä¸Šé‡‡æ ·) ---
        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(mamba_dim, mamba_dim, kernel_size=(1,4,4), stride=(1,2,2), padding=(0,1,1)),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )
        # Fusion Level 2 (åœ¨ 30x30 å°ºåº¦èåˆ)
        self.fusion_lvl2 = nn.Sequential(
            nn.Conv3d(mamba_dim * 2, mamba_dim, kernel_size=1, bias=False),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )

        # --- Level 2 Up: 30 -> 60 ---
        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(mamba_dim, mamba_dim, kernel_size=(1,4,4), stride=(1,2,2), padding=(0,1,1)),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )
        # Fusion Level 1 (åœ¨ 60x60 å°ºåº¦èåˆ)
        self.fusion_lvl1 = nn.Sequential(
            nn.Conv3d(mamba_dim * 2, mamba_dim, kernel_size=1, bias=False),
            nn.GroupNorm(8, mamba_dim),
            nn.GELU()
        )

        # --- Level 1 Up: 60 -> 120 ---
        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(mamba_dim, dim, kernel_size=(1,4,4), stride=(1,2,2), padding=(0,1,1)),
            # æœ€åä¸€æ¬¡ä¸Šé‡‡æ ·ä¸åŠ æ¿€æ´»ï¼Œä¿æŒç‰¹å¾çº¿æ€§ï¼Œæ–¹ä¾¿ä¸ Body ç‰¹å¾èåˆ
        )

        # U-Net Body Skip Fusion (120x120)
        self.skip_fusion = nn.Sequential(
            nn.Conv3d(dim * 2, dim, kernel_size=1, bias=False),
            nn.GroupNorm(8, dim),
            nn.GELU()
        )

        # ===========================
        # 5. Tail (è§£ç ä¸è¾“å‡º)
        # ===========================
        self.tail = nn.Sequential(
            nn.Conv3d(dim, dim, 3, padding=1),
            nn.GELU(),
            nn.Conv3d(dim, main_c, 1),
        )
        
        # ç¯å¢ƒè®¾ç½®
        self.use_channels_last_3d = os.environ.get("USE_CHANNELS_LAST_3D", "0") == "1"
        if self.use_channels_last_3d:
            self.to(memory_format=torch.channels_last_3d)

        self.aux_thr = float(os.environ.get("AUX_PRIOR_THR", "1e-6"))
        self._init_weights_logic()

    def _init_weights_logic(self):
        # [ä¿®æ”¹] æ®‹å·®å­¦ä¹ æ¨¡å¼åˆå§‹åŒ–
        # æœ€åä¸€å±‚åˆå§‹åŒ–ä¸º 0ï¼Œä½¿å¾—åˆå§‹çŠ¶æ€ä¸‹ Residual=0ï¼ŒOutput=Input(Base)
        last_layer = self.tail[-1]
        if isinstance(last_layer, nn.Conv3d):
            nn.init.constant_(last_layer.weight, 0.0)
            nn.init.constant_(last_layer.bias, 0.0)

    def _forward_mamba_stage(self, x, layers):
        """
        Helper function to handle flatten -> mamba -> unflatten
        x: [B, C, T, H, W]
        """
        B, C, T, H, W = x.shape
        x_flat = x.view(B, C, -1).transpose(1, 2).contiguous() # [B, L, C]
        
        for layer in layers:
            # éšå¼å±‚çº§æ®‹å·®è¿æ¥ (x + layer(x)) 
            # è¿™ä¿è¯äº†æ¯å±‚ Mamba éƒ½åœ¨å­¦ä¹ å¢é‡
            x_flat = x_flat + layer(x_flat)
            
        out = x_flat.transpose(1, 2).view(B, C, T, H, W).contiguous()
        return out

    def _build_allow_mask(self, aux, main):
        if aux.shape[1] >= 7:
            aux_prior = (aux[:, 0:1, ...] + aux[:, 6:7, ...]) * 0.5
        else:
            aux_prior = aux[:, 0:1, ...]
        return torch.clamp((main > 0).float() + (aux_prior > self.aux_thr).float(), 0.0, 1.0)

    def forward(self, aux, main):
        if self.use_channels_last_3d:
            aux = aux.to(memory_format=torch.channels_last_3d)
            main = main.to(memory_format=torch.channels_last_3d)

        main_norm = main / self.norm_const
        
        # ===========================
        # Stage 1: Encoding
        # ===========================
        aux_feat = self.aux_head(aux)
        aux_feat = self.aux_multiscale(aux_feat)
        main_feat = self.main_head(main_norm)

        # ğŸš€ [Global Residual] ä¿å­˜æµ…å±‚åŸºåº•ç‰¹å¾ (120x120)
        shallow_feat = main_feat

        # ===========================
        # Stage 2: Fusion
        # ===========================
        x = self.sft1(main_feat, aux_feat)
        x = self.gated_fusion(x, aux_feat)

        # ===========================
        # Stage 3: Deep Body (RIR)
        # ===========================
        for block in self.res_blocks:
            x = block(x) 
        
        # [New] RIR (Residual in Residual) è¿æ¥
        # Body Output = Conv(Blocks(x)) + Shallow Input
        x = self.body_tail(x) + shallow_feat
        
        # ä¿å­˜é«˜æ¸…ç‰¹å¾ç”¨äºæœ€å¤–å±‚ U-Net Skip
        x_high_res_skip = x

        # ===========================
        # Stage 4: Hierarchical Mamba Bottleneck (3-Level U-Net)
        # ===========================
        
        # --- 1. Level 1 Down: 120 -> 60 ---
        x_60_raw = self.down1(x)
        # Mamba Stage 1 (Capture Medium Freq)
        x_60 = self._forward_mamba_stage(x_60_raw, self.mamba_stage1)
        
        # --- 2. Level 2 Down: 60 -> 30 ---
        x_30_raw = self.down2(x_60)
        # Mamba Stage 2 (Capture Low Freq)
        x_30 = self._forward_mamba_stage(x_30_raw, self.mamba_stage2)
        
        # --- 3. Level 3 Down: 30 -> 15 ---
        x_15_raw = self.down3(x_30)
        # Mamba Stage 3 (Global Context / God's View)
        x_15 = self._forward_mamba_stage(x_15_raw, self.mamba_stage3)
        
        # --- 4. Level 3 Up: 15 -> 30 ---
        x_30_up = self.up3(x_15)
        # Fusion at 30x30 (Skip Connection from x_30)
        x_30_fused = self.fusion_lvl2(torch.cat([x_30_up, x_30], dim=1))
        
        # --- 5. Level 2 Up: 30 -> 60 ---
        x_60_up = self.up2(x_30_fused)
        # Fusion at 60x60 (Skip Connection from x_60)
        x_60_fused = self.fusion_lvl1(torch.cat([x_60_up, x_60], dim=1))
        
        # --- 6. Level 1 Up: 60 -> 120 ---
        x_120_out = self.up1(x_60_fused)

        # --- 7. Final Fusion with Body Features ---
        x_cat = torch.cat([x_high_res_skip, x_120_out], dim=1)
        x = self.skip_fusion(x_cat)

        # ===========================
        # Stage 5: Decoding
        # ===========================
        x = self.moe_block(x)          
        x = self.global_context(x)     
        x = self.sft2(x, aux_feat)     
        
        # ===========================
        # Stage 6: Output & Global Residual
        # ===========================
        # 1. è®¡ç®—é«˜é¢‘æ®‹å·®å›¾ (Residual Map)
        # æ­¤æ—¶ x çš„æ•°å€¼å¯æ­£å¯è´Ÿ
        residual = self.tail(x)
        
        # 2. å…¨å±€æ®‹å·®ç›¸åŠ  (Additive)
        # æœ€ç»ˆé¢„æµ‹ = å¹³æ»‘åŸºåº• + é«˜é¢‘æ®‹å·®
        pred = main_norm + residual
        
        # 3. ç‰©ç†çº¦æŸ
        allow_mask = self._build_allow_mask(aux, main_norm)
        pred = pred * allow_mask * self.norm_const
        
        # ä¿è¯éè´Ÿ
        pred = F.relu(pred)
        
        return pred, pred