import torch
import torch.nn as nn
import torch.nn.functional as F

# --- A. å¤šå°ºåº¦æ„ŸçŸ¥æ¨¡å— (Multi-Scale Block) ---
# ä¿æŒä¸å˜
class MultiScaleBlock3D(nn.Module):
    def __init__(self, channels):
        super().__init__()
        hid_c = channels // 4
        self.branch1 = nn.Conv3d(channels, hid_c, 3, 1, 1, dilation=1)
        self.branch2 = nn.Conv3d(channels, hid_c, 3, 1, 2, dilation=2)
        self.branch3 = nn.Conv3d(channels, hid_c, 3, 1, 4, dilation=4)
        self.branch4 = nn.Conv3d(channels, hid_c, 1, 1, 0)
        self.fusion = nn.Conv3d(channels, channels, 1, 1, 0)

    def forward(self, x):
        b1 = F.relu(self.branch1(x))
        b2 = F.relu(self.branch2(x))
        b3 = F.relu(self.branch3(x))
        b4 = F.relu(self.branch4(x))
        out = torch.cat([b1, b2, b3, b4], dim=1)
        return self.fusion(out) + x

# --- B. SFT èåˆå±‚ ---
# ä¿æŒä¸å˜
class SFTLayer3D(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.sft_net = nn.Sequential(
            nn.Conv3d(channels, channels, 3, 1, 1),
            nn.LeakyReLU(0.1),
            nn.Conv3d(channels, channels*2, 3, 1, 1)
        )
    def forward(self, main, aux):
        scale_shift = self.sft_net(aux)
        scale, shift = torch.chunk(scale_shift, 2, dim=1)
        return main * (1 + scale) + shift

# --- C. [ä¿®æ”¹ç‰ˆ] é«˜æ•ˆå…¨å±€æ³¨æ„åŠ› (Efficient Global Context) ---
# æ›¿æ¢æ‰äº†åŸæ¥é‚£ä¸ªç‚¸æ˜¾å­˜çš„ Transformer
# ä½œç”¨ï¼šå…ˆæŠŠå›¾å˜å°(Pooling)å†ç®—æ³¨æ„åŠ›ï¼Œç®—å®Œå†æ’å€¼å›å»
class EfficientContextBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # 1. é™ç»´ï¼Œå‡å°‘è®¡ç®—é‡
        self.reduce_conv = nn.Conv3d(dim, dim // 2, 1)
        
        # 2. å…¨å±€æ± åŒ– (æŠŠ 128x128 å˜æˆ 1x1 çš„ç‚¹ï¼Œè·å–å…¨å±€ä¿¡æ¯)
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        
        # 3. æ¿€åŠ±ç½‘ç»œ (ç±»ä¼¼ SE-Block)
        self.mlp = nn.Sequential(
            nn.Linear(dim // 2, dim // 2),
            nn.ReLU(),
            nn.Linear(dim // 2, dim),
            nn.Sigmoid()
        )
        
        # 4. æ¢å¤
        self.restore_conv = nn.Conv3d(dim, dim, 1)

    def forward(self, x):
        # x: [B, C, T, H, W]
        b, c, t, h, w = x.shape
        
        # æ®‹å·®è¿æ¥è¾“å…¥
        identity = x
        
        # 1. é™ä½é€šé“
        y = self.reduce_conv(x) # [B, C/2, T, H, W]
        
        # 2. å…¨å±€å¹³å‡æ± åŒ– -> å˜æˆä¸€ä¸ªå‘é‡
        y = self.avg_pool(y).view(b, -1) # [B, C/2]
        
        # 3. è®¡ç®—å…¨å±€æƒé‡
        y = self.mlp(y).view(b, c, 1, 1, 1) # [B, C, 1, 1, 1]
        
        # 4. æŠŠæƒé‡ä¹˜å›åŸå›¾ (Excite)
        out = x * y
        
        return self.restore_conv(out) + identity

# ==========================================
# â• æ–°å¢ï¼šSEN2SR æ ¸å¿ƒ - é¢‘ç‡ç¡¬çº¦æŸå±‚
# ==========================================
import torch.fft

class FrequencyHardConstraint(nn.Module):
    """
    å®ç° SEN2SR çš„æ ¸å¿ƒé€»è¾‘ï¼š
    åœ¨é¢‘åŸŸä¸­ï¼Œå¼ºè¡ŒæŠŠã€è¾“å…¥çš„ä½é¢‘ä¿¡æ¯ã€‘å’Œã€æ¨¡å‹çš„é¢„æµ‹é«˜é¢‘ä¿¡æ¯ã€‘æ‹¼æ¥ã€‚
    ä¿è¯ï¼šå®è§‚æ•°å€¼ï¼ˆä½é¢‘ï¼‰ç»ä¸å¤±çœŸï¼Œåªç”Ÿæˆçº¹ç†ï¼ˆé«˜é¢‘ï¼‰ã€‚
    """
    def __init__(self, radius=16):
        super().__init__()
        self.radius = radius # æ§åˆ¶ä¿ç•™å¤šå°‘ä½é¢‘ä¿¡æ¯ï¼ˆåŠå¾„è¶Šå°ï¼Œä¿ç•™çš„ä½é¢‘è¶Šå°‘ï¼‰

    def get_low_pass_filter(self, shape, device):
        # åˆ›å»ºä¸€ä¸ªåœ†å½¢çš„ä½é€šæ»¤æ³¢å™¨æ©è†œ (Mask)
        # 1 = ä¿ç•™ä½é¢‘ (ç”¨è¾“å…¥çš„), 0 = ä¿ç•™é«˜é¢‘ (ç”¨é¢„æµ‹çš„)
        b, c, t, h, w = shape
        center_h, center_w = h // 2, w // 2
        
        # ç”Ÿæˆç½‘æ ¼åæ ‡
        y = torch.arange(h, device=device)
        x = torch.arange(w, device=device)
        grid_y, grid_x = torch.meshgrid(y, x, indexing='ij')
        
        # è®¡ç®—è·ç¦»ä¸­å¿ƒçš„è·ç¦»
        dist = (grid_x - center_w)**2 + (grid_y - center_h)**2
        
        # ç”Ÿæˆ Mask
        mask = torch.zeros((h, w), device=device)
        mask[dist <= self.radius**2] = 1.0
        
        # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é… [B, C, T, H, W]
        return mask.view(1, 1, 1, h, w)

    def forward(self, pred, input_main):
        """
        pred: æ¨¡å‹é¢„æµ‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒ (åŒ…å«å¯èƒ½çš„é”™è¯¯ä½é¢‘)
        input_main: åŸå§‹çš„ç²—ç³™è¾“å…¥ (ä½é¢‘æ˜¯ç»å¯¹å‡†ç¡®çš„)
        """
        # 1. ç¡®ä¿è¾“å…¥å°ºå¯¸ä¸€è‡´ (é€šå¸¸ input_main å·²ç»æ˜¯ 128x128 çš„æ’å€¼ç»“æœ)
        if pred.shape != input_main.shape:
            input_main = F.interpolate(
                input_main.view(input_main.shape[0], -1, input_main.shape[3], input_main.shape[4]),
                size=pred.shape[-2:], mode='bilinear', align_corners=False
            ).view_as(pred)

        # 2. è½¬åˆ°é¢‘åŸŸ (FFT)
        # åªåœ¨ç©ºé—´ç»´åº¦ (H, W) ä¸Šåš FFT
        fft_pred = torch.fft.fftn(pred, dim=(-2, -1))
        fft_input = torch.fft.fftn(input_main, dim=(-2, -1))
        
        # ç§»é¢‘ (æŠŠä½é¢‘ç§»åˆ°å›¾åƒä¸­å¿ƒ)
        fft_pred_shift = torch.fft.fftshift(fft_pred, dim=(-2, -1))
        fft_input_shift = torch.fft.fftshift(fft_input, dim=(-2, -1))
        
        # 3. è·å–æ»¤æ³¢å™¨æ©è†œ
        mask = self.get_low_pass_filter(pred.shape, pred.device)
        
        # 4. æ ¸å¿ƒæ“ä½œï¼šèåˆ
        # MaskåŒºåŸŸ(ä½é¢‘): ä½¿ç”¨ input çš„çœŸå®ä¿¡æ¯
        # éMaskåŒºåŸŸ(é«˜é¢‘): ä½¿ç”¨ pred çš„ç”Ÿæˆä¿¡æ¯
        fft_fused_shift = fft_input_shift * mask + fft_pred_shift * (1 - mask)
        
        # 5. é€†å˜æ¢å›ç©ºåŸŸ (IFFT)
        fft_fused = torch.fft.ifftshift(fft_fused_shift, dim=(-2, -1))
        output = torch.fft.ifftn(fft_fused, dim=(-2, -1)).real
        
        return output
    

    # [è¿½åŠ åˆ° models/blocks.py]

# ==========================================
# ğŸ†• æ–°å¢æ¨¡å— 1: MoE (Mixture of Experts)
# ==========================================
class MoEBlock(nn.Module):
    """
    ç¨€ç–æ··åˆä¸“å®¶æ¨¡å— (Sparse MoE)
    ç”¨é€”ï¼šæ›¿æ¢æ™®é€šå·ç§¯å±‚ï¼Œå¢åŠ æ¨¡å‹å®¹é‡ä½†ä¿æŒä½è®¡ç®—é‡ã€‚
    """
    def __init__(self, dim, num_experts=4, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        # é—¨æ§ç½‘ç»œ (Gating Network): å†³å®šç”¨å“ªä¸ªä¸“å®¶
        self.gate = nn.Linear(dim, num_experts)
        
        # ä¸“å®¶ç½‘ç»œ (Experts): è¿™é‡Œç”¨ç®€å•çš„ 1x1 å·ç§¯ä»£æ›¿ MLP
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Conv3d(dim, dim, 1),
                nn.PReLU(),
                nn.Conv3d(dim, dim, 1)
            ) for _ in range(num_experts)
        ])
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        b, c, t, h, w = x.shape
        
        # 1. è®¡ç®—è·¯ç”±æƒé‡
        # å…ˆæŠŠç©ºé—´æ—¶é—´ç»´åº¦å±•å¹³åš attention
        x_flat = x.permute(0, 2, 3, 4, 1).reshape(-1, c) # [N, C]
        logits = self.gate(x_flat) # [N, num_experts]
        
        # 2. é€‰å‡º Top-K ä¸“å®¶
        probs, indices = torch.topk(logits, self.top_k, dim=1)
        probs = F.softmax(probs, dim=1)
        
        # 3. åŠ¨æ€èšåˆ
        # ä¸ºäº†ä»£ç ç®€å•ä¸”å…¼å®¹æ€§å¥½ï¼Œè¿™é‡Œç”¨å¾ªç¯å®ç°ï¼ˆè™½ç„¶æ…¢ä¸€ç‚¹ç‚¹ï¼Œä½†ç¨³ï¼‰
        out = torch.zeros_like(x_flat)
        
        for k in range(self.top_k):
            expert_idx = indices[:, k] # å½“å‰ç¬¬kä¸ªé€‰æ‹©çš„ä¸“å®¶ç´¢å¼•
            prob = probs[:, k].unsqueeze(1) # æƒé‡
            
            # è¿™é‡Œçš„ mask ç¨å¾®æœ‰ç‚¹è€—æ—¶ï¼Œå·¥ç¨‹åŒ–é€šå¸¸ä¼šç”¨ scatter
            # ä½†é’ˆå¯¹å° Batch è®­ç»ƒï¼Œç›´æ¥éå†ä¸“å®¶æ›´ç›´è§‚
            for i in range(self.num_experts):
                # æ‰¾åˆ°æ‰€æœ‰é€‰æ‹©äº†ä¸“å®¶ i çš„æ ·æœ¬ä½ç½®
                mask = (expert_idx == i)
                if mask.any():
                    # æå–è¿™äº›æ ·æœ¬
                    selected_x = x_flat[mask]
                    # è¿˜åŸæˆ 3D å½¢çŠ¶é€å…¥å·ç§¯ [Batch_sub, C, 1, 1, 1] æ¨¡æ‹Ÿ
                    # æ³¨æ„ï¼šä¸ºäº†è®© 3D å·ç§¯èƒ½å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦ reshape å›å»
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥ç”¨ Linear æ¨¡æ‹Ÿ 1x1 å·ç§¯æ•ˆæœ
                    # çœŸæ­£çš„ MoE å·ç§¯éœ€è¦æ›´å¤æ‚çš„ scatter/gather
                    # ä¸‹é¢æ˜¯é€»è¾‘ç­‰æ•ˆçš„ç®€åŒ–ç‰ˆï¼š
                    
                    # é‡æ–°æ„é€  expert çš„ forward
                    # ç”±äºæˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„æ˜¯ Conv3dï¼Œè¿™é‡Œä¸ºäº†å¯¹é½å½¢çŠ¶ï¼š
                    inp_sub = selected_x.view(-1, c, 1, 1, 1)
                    out_sub = self.experts[i](inp_sub).view(-1, c)
                    
                    # ç´¯åŠ ç»“æœ
                    out[mask] += out_sub * prob[mask]
                    
        return out.view(b, t, h, w, c).permute(0, 4, 1, 2, 3) + x # æ®‹å·®è¿æ¥

# ==========================================
# ğŸ†• æ–°å¢æ¨¡å— 2: Mamba-like Block (çº¯ PyTorch ç‰ˆ)
# ==========================================
class SimpleMambaBlock(nn.Module):
    """
    ç®€åŒ–ç‰ˆ Mamba å— (æ— éœ€å®‰è£… mamba-ssm åº“ï¼Œé€‚é… AMD)
    ä½¿ç”¨ é—¨æ§å·ç§¯ + æ·±åº¦å¯åˆ†ç¦»å·ç§¯ æ¨¡æ‹Ÿ SSM çš„é€‰æ‹©æ€§æœºåˆ¶
    """
    def __init__(self, dim, d_state=16):
        super().__init__()
        self.dim = dim
        
        # 1. è¾“å…¥æŠ•å½±
        self.in_proj = nn.Conv3d(dim, dim * 2, 1)
        
        # 2. æ·±åº¦å·ç§¯ (æ¨¡æ‹Ÿ SSM çš„é•¿æœŸè®°å¿†)
        self.conv = nn.Conv3d(dim, dim, 3, 1, 1, groups=dim)
        
        # 3. çŠ¶æ€æŠ•å½± (æ¨¡æ‹Ÿ SSM çš„å‚æ•°ç¦»æ•£åŒ–)
        self.x_proj = nn.Linear(dim, d_state + dim * 2) 
        
        # 4. é—¨æ§æœºåˆ¶
        self.act = nn.SiLU()
        
        # 5. è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Conv3d(dim, dim, 1)

    def forward(self, x):
        # x: [B, C, T, H, W]
        residual = x
        b, c, t, h, w = x.shape
        
        # 1. æŠ•å½±åˆ†ä¸ºä¸¤æ”¯: x (ä¿¡å·) å’Œ z (é—¨)
        xz = self.in_proj(x)
        x_signal, z_gate = torch.chunk(xz, 2, dim=1)
        
        # 2. å¤„ç†ä¿¡å·æ”¯ (Conv -> SiLU)
        x_signal = self.conv(x_signal)
        x_signal = self.act(x_signal)
        
        # 3. ç®€åŒ–ç‰ˆ SSM æ“ä½œ (ç”¨ é—¨æ§æ³¨æ„åŠ› æ¨¡æ‹Ÿ)
        # çœŸæ­£çš„ Mamba è¿™é‡Œæ˜¯ scan æ“ä½œï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨ Global Avg æ¨¡æ‹ŸçŠ¶æ€é€‰æ‹©
        x_flat = x_signal.mean(dim=[2,3,4]) # [B, C] å…¨å±€æè¿°ç¬¦
        params = self.x_proj(x_flat) # [B, d_state + 2*dim]
        
        # åŠ¨æ€è°ƒæ•´ç‰¹å¾ (Selective Scan çš„å¹³æ›¿)
        dt, B_state, C_state = torch.split(params, [c, c, 16], dim=1)
        dt = torch.sigmoid(dt).view(b, c, 1, 1, 1)
        
        y = x_signal * dt # è¿™ç§â€œè½¯é—¨æ§â€æ¨¡æ‹Ÿäº†é€‰æ‹©æ€§é—å¿˜
        
        # 4. é—¨æ§èåˆ
        y = y * self.act(z_gate)
        
        # 5. è¾“å‡º
        return self.out_proj(y) + residual