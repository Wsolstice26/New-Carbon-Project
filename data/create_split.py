import json
import random
import os
import numpy as np

# ==========================================
# âš™ï¸ 1. é…ç½®åŒºåŸŸ
# ==========================================
# ä¹‹å‰ preprocess_data.py æ‰«æå‡ºçš„æœ‰æ•ˆä½ç½®æ•°
NUM_TOTAL_PATCHES = 429 

# ğŸ”¥ [ä¿®æ­£] æ•°æ®å®é™…æ‰€åœ¨çš„æ–‡ä»¶å¤¹ (è™½ç„¶åå­—å« 120, ä½†é‡Œé¢å…¶å®æ˜¯ 160 çš„åˆ‡ç‰‡)
DATA_DIR = "/home/wdc/Carbon-Emission-Super-Resolution/data/Train_Data_Yearly_120"

# ç´¢å¼•é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„
OUTPUT_JSON = "/home/wdc/Carbon-Emission-Super-Resolution/Configs/split_config.json"

# åˆ’åˆ†æ¯”ä¾‹ 8:1:1
TRAIN_RATIO = 0.8
VAL_RATIO = 0.1

def create_indices():
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆæ ·æœ¬ç´¢å¼• (é¢„æœŸä½ç½®æ•°: {NUM_TOTAL_PATCHES})...")
    
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DATA_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç›®å½• {DATA_DIR}")
        return

    # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª X_*.npy æ–‡ä»¶æ¥éªŒè¯æ•°é‡
    import glob
    x_files = glob.glob(os.path.join(DATA_DIR, "X_*.npy"))
    if not x_files:
        print(f"âŒ é”™è¯¯: åœ¨ {DATA_DIR} ä¸‹æ‰¾ä¸åˆ°ä»»ä½• X_*.npy æ–‡ä»¶ï¼")
        return
    
    # ä½¿ç”¨æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶è¿›è¡ŒéªŒè¯
    sample_file = x_files[0]
    print(f"ğŸ” æ­£åœ¨é€šè¿‡æ–‡ä»¶éªŒè¯æ ·æœ¬æ•°: {os.path.basename(sample_file)}")
    
    try:
        temp_data = np.load(sample_file)
        actual_count = temp_data.shape[0]
        print(f"ğŸ“Š æ–‡ä»¶å†…å®é™…æ ·æœ¬æ•°: {actual_count}")
    except Exception as e:
        print(f"âŒ è¯»å– NPY æ–‡ä»¶å¤±è´¥: {e}")
        return

    # ä»¥å®é™…æ¢æµ‹åˆ°çš„æ•°é‡ä¸ºå‡†
    count = actual_count
    
    # --- æ ¸å¿ƒåˆ’åˆ†é€»è¾‘ ---
    indices = list(range(count))
    
    # éšæœºæ‰“ä¹± (å›ºå®šç§å­ 42)
    random.seed(42)
    random.shuffle(indices)
    
    # è®¡ç®—åˆ‡åˆ†ç‚¹
    train_end = int(count * TRAIN_RATIO)
    val_end = train_end + int(count * VAL_RATIO)
    
    # æ‰§è¡Œåˆ‡åˆ†
    train_indices = indices[:train_end]
    val_indices = indices[train_end:val_end]
    test_indices = indices[val_end:]
    
    # æ„å»ºé…ç½®å­—å…¸
    split_dict = {
        "metadata": {
            "patch_size": 160,
            "actual_data_dir": DATA_DIR, # è®°å½•çœŸå®è·¯å¾„
            "total_locations": count
        },
        "train_indices": train_indices,
        "val_indices": val_indices,
        "test_indices": test_indices
    }
    
    # ä¿å­˜ä¸º JSON
    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)
    with open(OUTPUT_JSON, 'w') as f:
        json.dump(split_dict, f, indent=4)
    
    print(f"\nâœ… ç´¢å¼•æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: {OUTPUT_JSON}")
    print(f"ğŸ“Š åˆ’åˆ†ç»“æœ:")
    print(f"   [è®­ç»ƒé›†]: {len(train_indices)} ä¸ªä½ç½®")
    print(f"   [éªŒè¯é›†]: {len(val_indices)} ä¸ªä½ç½®")
    print(f"   [æµ‹è¯•é›†]: {len(test_indices)} ä¸ªä½ç½®")
    print(f"\nğŸ‘‰ ä¸‹ä¸€æ­¥: è¯·ä¿®æ”¹ data/dataset.py ç¡®ä¿å®ƒä¹Ÿè¯»å–æ­£ç¡®çš„è·¯å¾„ã€‚")

if __name__ == "__main__":
    create_indices()