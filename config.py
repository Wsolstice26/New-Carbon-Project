import os
import json
import glob
from datetime import datetime

# ============================================================
# ğŸ›ï¸ å®éªŒå‚æ•°æ§åˆ¶å°
# ============================================================
RESUME = True  # å¦‚æœä½ æƒ³æ¥ç€ä¹‹å‰çš„æ¨¡å‹è·‘ï¼Œæ”¹æˆ True

# æ˜¾å­˜ä¼˜åŒ–é…ç½®
TARGET_BATCH_SIZE = 32
BATCH_SIZE = 8        
GRAD_ACCUM_STEPS = TARGET_BATCH_SIZE // BATCH_SIZE

PATCH_SIZE = 120
DIM = 64  
TIME_WINDOW = 3

# ğŸ·ï¸ å®éªŒæ ‡ç­¾ï¼šLinear Mode
TAG = "LinearLoss_WeightedL1_DirectR2"

# ============================================================
# ğŸ“‚ è‡ªåŠ¨è·¯å¾„ç”Ÿæˆç³»ç»Ÿ
# ============================================================
PROJECT_ROOT = "/home/wdc/Carbon-Emission-Super-Resolution"

identity_suffix = f"Size{PATCH_SIZE}_Dim{DIM}_EffBatch{TARGET_BATCH_SIZE}"
if TAG:
    identity_suffix += f"_{TAG}"

checkpoints_root = os.path.join(PROJECT_ROOT, "Checkpoints")
os.makedirs(checkpoints_root, exist_ok=True)

if not RESUME:
    current_time = datetime.now().strftime("%Y%m%d_%H%M")
    exp_name = f"Run_{current_time}_{identity_suffix}"
    print(f"ğŸ†• [New Experiment] åˆ›å»ºæ–°å®éªŒç›®å½•: {exp_name}")
else:
    search_pattern = os.path.join(checkpoints_root, f"Run_*_{identity_suffix}")
    candidates = glob.glob(search_pattern)
    if len(candidates) > 0:
        candidates.sort()
        latest_folder_path = candidates[-1]
        exp_name = os.path.basename(latest_folder_path)
        print(f"ğŸ”„ [Resume] è‡ªåŠ¨å®šä½åˆ°æœ€è¿‘çš„å®éªŒ: {exp_name}")
    else:
        raise FileNotFoundError(f"âŒ æ— æ³• Resumeï¼šæœªæ‰¾åˆ°å‚æ•°åŒ¹é…çš„æ—§å®éªŒæ–‡ä»¶å¤¹ã€‚\næœç´¢æ¨¡å¼: {search_pattern}")

SAVE_DIR = os.path.join(checkpoints_root, exp_name)
DATA_DIR = os.path.join(PROJECT_ROOT, "data", f"Train_Data_Yearly_{PATCH_SIZE}")

# ============================================================
# âš™ï¸ æœ€ç»ˆé…ç½®å­—å…¸
# ============================================================
CONFIG = {
    "project_root": PROJECT_ROOT,
    "data_dir": DATA_DIR,
    "save_dir": SAVE_DIR,
    "split_config": os.path.join(PROJECT_ROOT, "Configs", "split_config.json"),

    # Data / Model
    "patch_size": PATCH_SIZE,
    "dim": DIM,
    "batch_size": BATCH_SIZE, 
    "grad_accum_steps": GRAD_ACCUM_STEPS,
    "target_batch_size": TARGET_BATCH_SIZE,
    "time_window": TIME_WINDOW,
    "consistency_scale": 10,
    
    # æ·±åº¦æ§åˆ¶å‚æ•°
    "num_mamba_layers": 2,  
    "num_res_blocks": 4,    

    # Training
    "epochs": 500,
    "num_workers": 6,
    
    # ğŸš€ [LR è°ƒæ•´] çº¿æ€§ Loss å»ºè®®ä» 1e-4 å¼€å§‹
    "lr": 1e-4,                 
    "main_metric": "r2_score",  

    "patience": 50, # çº¿æ€§ Loss æ”¶æ•›å¿«ï¼Œè€å¿ƒå¯ä»¥ç»™å°ç‚¹

    # Loss æƒé‡ (Log ç›¸å…³çš„å‚æ•°å·²ç§»é™¤)
    "w_sparse": 1e-3,           
    "w_ent": 1e-3,              
    "ent_mode": "max",
    "target_entropy": 1.5,
    "use_charbonnier_A": False,

    "resume": RESUME,
    "seed": 42,
    "deterministic": True,
    
    # ğŸš€ [Norm è°ƒæ•´] æ¨¡å‹è¾“å‡ºç›´æ¥å¯¹é½ dataset é‡Œçš„ /1000 æ•°æ®
    "norm_factor": 1.0,  
    "device": "cuda",

    "save_every_steps": 200,
    "keep_last_steps": 5,
    "save_every_epochs": 10,
    "save_epoch_model_only": False,
}

os.makedirs(SAVE_DIR, exist_ok=True)
config_save_path = os.path.join(SAVE_DIR, "experiment_config.json")
with open(config_save_path, "w") as f:
    json.dump({k: v for k, v in CONFIG.items() if isinstance(v, (str, int, float, bool))}, f, indent=4)

print(f"âœ… é…ç½®å·²åŠ è½½ | ä¿å­˜è·¯å¾„: {SAVE_DIR}")
print(f"ğŸ”¥ æ¨¡å¼: çº¯çº¿æ€§å›å½’ (Weighted L1) | LR: {CONFIG['lr']}")